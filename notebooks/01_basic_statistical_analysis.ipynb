{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistical Analysis with Monet Stats\n",
    "\n",
    "This notebook demonstrates the basic statistical analysis capabilities of Monet Stats for atmospheric sciences. We'll explore fundamental metrics for model evaluation and verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# For xarray support\n",
    "import monet_stats as ms\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Example Dataset\n",
    "\n",
    "We'll use the synthetic temperature dataset generated for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (36530, 6)\n",
      "\n",
      "First few rows:\n",
      "         date station_id   latitude   longitude  observed_temp  modeled_temp\n",
      "0  2010-01-01     STN001  48.807459 -104.292642       4.846258      2.405771\n",
      "1  2010-01-01     STN002  41.618037  -71.091006       7.334191      4.291591\n",
      "2  2010-01-01     STN003  44.480624  -77.625068       3.787901      2.090386\n",
      "3  2010-01-01     STN004  37.405214 -102.395090       8.378226      5.821626\n",
      "4  2010-01-01     STN005  30.463983  -90.015159       1.589425      0.320924\n",
      "\n",
      "Dataset summary:\n",
      "           latitude     longitude  observed_temp  modeled_temp\n",
      "count  36530.000000  36530.000000   36530.000000  36530.000000\n",
      "mean      38.944442    -84.798976      15.217608     13.173439\n",
      "std        5.563585     13.353808       7.306792      7.250814\n",
      "min       30.399022   -104.292642      -0.789027     -1.714732\n",
      "25%       36.541071    -98.972585       8.431574      6.334459\n",
      "50%       38.742505    -83.820113      15.220243     13.161681\n",
      "75%       42.867920    -71.091006      22.030278     20.010554\n",
      "max       48.807459    -70.198102      31.457807     27.876271\n"
     ]
    }
   ],
   "source": [
    "# Load the example temperature dataset\n",
    "temp_df = pd.read_csv('../data/temperature_obs_mod.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", temp_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(temp_df.head())\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "print(temp_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Error Metrics\n",
    "\n",
    "Let's compute basic error metrics between observed and modeled temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Error Metrics:\n",
      "Mean Absolute Error (MAE): 2.044\n",
      "Root Mean Square Error (RMSE): 2.130\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'monet_stats' has no attribute 'MBE'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean Absolute Error (MAE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mms.MAE(obs_temps,\u001b[38;5;250m \u001b[39mmod_temps)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRoot Mean Square Error (RMSE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mms.RMSE(obs_temps,\u001b[38;5;250m \u001b[39mmod_temps)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean Bias Error (MBE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMBE\u001b[49m(obs_temps,\u001b[38;5;250m \u001b[39mmod_temps)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean Absolute Percentage Error (MAPE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mms.MAPE(obs_temps,\u001b[38;5;250m \u001b[39mmod_temps)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean Percentage Error (MPE): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mms.MPE(obs_temps,\u001b[38;5;250m \u001b[39mmod_temps)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'monet_stats' has no attribute 'MBE'"
     ]
    }
   ],
   "source": [
    "# Extract observed and modeled temperatures\n",
    "obs_temps = temp_df['observed_temp'].values\n",
    "mod_temps = temp_df['modeled_temp'].values\n",
    "\n",
    "# Compute basic error metrics\n",
    "print(\"Basic Error Metrics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {ms.MAE(obs_temps, mod_temps):.3f}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {ms.RMSE(obs_temps, mod_temps):.3f}\")\n",
    "print(f\"Mean Bias Error (MB): {ms.MB(obs_temps, mod_temps):.3f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {ms.MAPE(obs_temps, mod_temps):.3f}%\")\n",
    "print(f\"Mean Percentage Error (MPE): {ms.MPE(obs_temps, mod_temps):.3f}%\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "print(\"\\nCorrelation Metrics:\")\n",
    "print(f\"Pearson Correlation: {ms.pearson_correlation(obs_temps, mod_temps):.3f}\")\n",
    "print(f\"Spearman Correlation: {ms.spearman_correlation(obs_temps, mod_temps):.3f}\")\n",
    "print(f\"Coefficient of Determination (R²): {ms.R2(obs_temps, mod_temps):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Scores\n",
    "\n",
    "Calculate skill scores relative to a reference forecast (e.g., climatology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skill scores\n",
    "print(\"Skill Scores (relative to climatology reference):\")\n",
    "\n",
    "# Use overall mean as climatology reference\n",
    "climatology = np.mean(obs_temps)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_model = ms.RMSE(obs_temps, mod_temps)\n",
    "rmse_climo = ms.RMSE(obs_temps, np.full_like(obs_temps, climatology))\n",
    "\n",
    "# Calculate skill score\n",
    "ss_rmse = ms.SS(mse_model=rmse_model**2, mse_reference=rmse_climo**2)\n",
    "print(f\"RMSE Skill Score: {ss_rmse:.3f}\")\n",
    "\n",
    "# Nash-Sutcliffe Efficiency\n",
    "nse = ms.NSE(obs_temps, mod_temps)\n",
    "print(f\"Nash-Sutcliffe Efficiency: {nse:.3f}\")\n",
    "\n",
    "# Modified Nash-Sutcliffe Efficiency\n",
    "mnse = ms.mNSE(obs_temps, mod_temps)\n",
    "print(f\"Modified Nash-Sutcliffe Efficiency: {mnse:.3f}\")\n",
    "\n",
    "# Relative Nash-Sutcliffe Efficiency\n",
    "rnse = ms.rNSE(obs_temps, mod_temps)\n",
    "print(f\"Relative Nash-Sutcliffe Efficiency: {rnse:.3f}\")\n",
    "\n",
    "# Nash-Sutcliffe Efficiency (log)\n",
    "nse_log = ms.NSElog(obs_temps, mod_temps)\n",
    "print(f\"Nash-Sutcliffe Efficiency (log): {nse_log:.3f}\")\n",
    "\n",
    "# Nash-Sutcliffe Efficiency (modified)\n",
    "nse_m = ms.NSEm(obs_temps, mod_temps)\n",
    "print(f\"Nash-Sutcliffe Efficiency (modified): {nse_m:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Create visualizations to understand the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0, 0].scatter(obs_temps[:1000], mod_temps[:1000], alpha=0.5, s=20)\n",
    "axes[0, 0].plot([obs_temps.min(), obs_temps.max()], [obs_temps.min(), obs_temps.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Observed Temperature (°C)')\n",
    "axes[0, 0].set_ylabel('Modeled Temperature (°C)')\n",
    "axes[0, 0].set_title(f'Scatter Plot (R² = {ms.R2(obs_temps, mod_temps):.3f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time series for first station\n",
    "first_station_data = temp_df[temp_df['station_id'] == temp_df['station_id'].iloc[0]]\n",
    "axes[0, 1].plot(first_station_data['date'][:365], first_station_data['observed_temp'][:365], label='Observed', alpha=0.7)\n",
    "axes[0, 1].plot(first_station_data['date'][:365], first_station_data['modeled_temp'][:365], label='Modeled', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Temperature (°C)')\n",
    "axes[0, 1].set_title('Time Series Comparison (First Station, First Year)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Histogram of errors\n",
    "errors = mod_temps - obs_temps\n",
    "axes[1, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Model Error (°C)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Distribution of Errors (Mean: {np.mean(errors):.3f}, Std: {np.std(errors):.3f})')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot to check normality of errors\n",
    "from scipy import stats\n",
    "\n",
    "stats.probplot(errors, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot of Errors')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station-wise Analysis\n",
    "\n",
    "Analyze model performance by station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each station\n",
    "stations = temp_df['station_id'].unique()\n",
    "station_metrics = []\n",
    "\n",
    "for station in stations[:5]:  # Analyze first 5 stations\n",
    "    station_data = temp_df[temp_df['station_id'] == station]\n",
    "    obs_station = station_data['observed_temp'].values\n",
    "    mod_station = station_data['modeled_temp'].values\n",
    "\n",
    "    station_metrics.append({\n",
    "        'station': station,\n",
    "        'MAE': ms.MAE(obs_station, mod_station),\n",
    "        'RMSE': ms.RMSE(obs_station, mod_station),\n",
    "        'R2': ms.R2(obs_station, mod_station),\n",
    "        'Correlation': ms.pearson_correlation(obs_station, mod_station),\n",
    "        'MBE': ms.MBE(obs_station, mod_station)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "metrics_df = pd.DataFrame(station_metrics)\n",
    "print(\"Station-wise Performance Metrics:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualize station performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(metrics_df['station'], metrics_df['RMSE'])\n",
    "axes[0].set_xlabel('Station')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].set_title('RMSE by Station')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].bar(metrics_df['station'], metrics_df['R2'])\n",
    "axes[1].set_xlabel('Station')\n",
    "axes[1].set_ylabel('R²')\n",
    "axes[1].set_title('R² by Station')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the basic statistical analysis capabilities of Monet Stats:\n",
    "\n",
    "1. **Error Metrics**: MAE, RMSE, MBE, MAPE, MPE\n",
    "2. **Correlation Metrics**: Pearson, Spearman, R²\n",
    "3. **Skill Scores**: NSE, mNSE, rNSE, NSElog, NSEm, SS\n",
    "4. **Visualization**: Scatter plots, time series, error distributions\n",
    "5. **Station-wise Analysis**: Performance by location\n",
    "\n",
    "These metrics provide a comprehensive view of model performance for temperature forecasts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
