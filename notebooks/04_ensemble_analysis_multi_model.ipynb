{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Analysis and Multi-Model Evaluation with Monet Stats\n",
    "\n",
    "This notebook demonstrates ensemble forecasting analysis and multi-model evaluation techniques using Monet Stats. We'll work with ensemble forecasts and explore various probabilistic verification methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# For xarray support\n",
    "import monet_stats as ms\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Temperature Dataset with Ensemble Forecasts\n",
    "\n",
    "We'll use the synthetic temperature dataset that includes ensemble forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example temperature dataset\n",
    "temp_df = pd.read_csv('../data/temperature_obs_mod.csv')\n",
    "\n",
    "# For ensemble analysis, we'll generate synthetic ensemble forecasts\n",
    "# based on the existing observed and modeled data\n",
    "obs_temps = temp_df['observed_temp'].values\n",
    "mod_temps = temp_df['modeled_temp'].values\n",
    "\n",
    "print(\"Dataset shape:\", temp_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(temp_df.head())\n",
    "\n",
    "# Generate synthetic ensemble forecasts\n",
    "n_members = 10 # Number of ensemble members\n",
    "n_samples = len(obs_temps)\n",
    "\n",
    "# Create ensemble forecasts with systematic differences and random variations\n",
    "ensemble_forecasts = np.zeros((n_samples, n_members))\n",
    "for i in range(n_members):\n",
    "    # Each ensemble member has slightly different bias and random error\n",
    "    member_bias = np.random.normal(0, 0.5)  # Random bias\n",
    "    member_noise = np.random.normal(0, np.random.uniform(0.5, 1.5), n_samples)  # Random noise\n",
    "\n",
    "    # Ensemble member forecast\n",
    "    ensemble_forecasts[:, i] = mod_temps + member_bias + member_noise\n",
    "\n",
    "print(f\"\\nGenerated ensemble forecasts with {n_members} members\")\n",
    "print(f\"Ensemble shape: {ensemble_forecasts.shape}\")\n",
    "print(f\"Ensemble mean bias: {np.mean(np.mean(ensemble_forecasts, axis=1) - obs_temps):.4f}\")\n",
    "print(f\"Ensemble spread (mean std): {np.mean(np.std(ensemble_forecasts, axis=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Statistics\n",
    "\n",
    "Calculate basic ensemble statistics and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ensemble statistics\n",
    "ensemble_mean = np.mean(ensemble_forecasts, axis=1)\n",
    "ensemble_std = np.std(ensemble_forecasts, axis=1)\n",
    "ensemble_min = np.min(ensemble_forecasts, axis=1)\n",
    "ensemble_max = np.max(ensemble_forecasts, axis=1)\n",
    "\n",
    "print(\"Ensemble Statistics:\")\n",
    "print(f\"Mean of ensemble means: {np.mean(ensemble_mean):.4f}\")\n",
    "print(f\"Mean of ensemble std: {np.mean(ensemble_std):.4f}\")\n",
    "print(f\"Overall ensemble range: {np.min(ensemble_min):.4f} to {np.max(ensemble_max):.4f}\")\n",
    "\n",
    "# Calculate metrics for ensemble mean vs observations\n",
    "print(\"\\nEnsemble Mean Performance:\")\n",
    "print(f\"MAE (Ensemble Mean vs Obs): {ms.MAE(obs_temps, ensemble_mean):.4f}\")\n",
    "print(f\"RMSE (Ensemble Mean vs Obs): {ms.RMSE(obs_temps, ensemble_mean):.4f}\")\n",
    "print(f\"MBE (Ensemble Mean vs Obs): {ms.MBE(obs_temps, ensemble_mean):.4f}\")\n",
    "print(f\"Correlation (Ensemble Mean vs Obs): {ms.pearson_correlation(obs_temps, ensemble_mean):.4f}\")\n",
    "print(f\"R² (Ensemble Mean vs Obs): {ms.R2(obs_temps, ensemble_mean):.4f}\")\n",
    "\n",
    "# Calculate metrics for single model vs observations\n",
    "print(\"\\nSingle Model Performance:\")\n",
    "print(f\"MAE (Model vs Obs): {ms.MAE(obs_temps, mod_temps):.4f}\")\n",
    "print(f\"RMSE (Model vs Obs): {ms.RMSE(obs_temps, mod_temps):.4f}\")\n",
    "print(f\"MBE (Model vs Obs): {ms.MBE(obs_temps, mod_temps):.4f}\")\n",
    "print(f\"Correlation (Model vs Obs): {ms.pearson_correlation(obs_temps, mod_temps):.4f}\")\n",
    "print(f\"R² (Model vs Obs): {ms.R2(obs_temps, mod_temps):.4f}\")\n",
    "\n",
    "# Compare ensemble vs single model performance\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Ensemble vs Single Model RMSE Improvement: {(ms.RMSE(obs_temps, mod_temps) - ms.RMSE(obs_temps, ensemble_mean)) / ms.RMSE(obs_temps, mod_temps) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Verification Metrics\n",
    "\n",
    "Calculate probabilistic verification metrics for ensemble forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilistic verification metrics\n",
    "print(\"Probabilistic Verification Metrics:\")\n",
    "\n",
    "# Brier Score for different temperature thresholds\n",
    "thresholds = [0, 10, 20, 30]  # Temperature thresholds in Celsius\n",
    "for thresh in thresholds:\n",
    "    # Binary observation (1 if temp > threshold, 0 otherwise)\n",
    "    obs_binary = (obs_temps > thresh).astype(int)\n",
    "\n",
    "    # Probability from ensemble (fraction of members above threshold)\n",
    "    prob_forecast = np.mean(ensemble_forecasts > thresh, axis=1)\n",
    "\n",
    "    # Calculate Brier Score\n",
    "    bs = ms.BS(obs_binary, prob_forecast)\n",
    "    print(f\"Brier Score (T > {thresh}°C): {bs:.4f}\")\n",
    "\n",
    "# Continuous Ranked Probability Score (CRPS)\n",
    "# Since we don't have the exact implementation in monet_stats, we'll use a simplified approach\n",
    "# For demonstration purposes, we'll calculate a related metric\n",
    "print(\"\\nEnsemble Spread vs Error:\")\n",
    "abs_errors = np.abs(ensemble_mean - obs_temps)\n",
    "spread_error_corr = ms.pearson_correlation(ensemble_std, abs_errors)\n",
    "print(f\"Correlation between ensemble spread and absolute error: {spread_error_corr:.4f}\")\n",
    "\n",
    "# Reliability analysis\n",
    "def reliability_analysis(obs, prob_forecasts, n_bins=10):\n",
    "    \"\"\"Perform reliability analysis\"\"\"\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    observed_freq = []\n",
    "    forecast_prob = []\n",
    "    bin_counts = []\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = (prob_forecasts >= bin_edges[i]) & (prob_forecasts < bin_edges[i+1])\n",
    "        if i == n_bins - 1:  # Include upper bound for last bin\n",
    "            bin_mask = (prob_forecasts >= bin_edges[i]) & (prob_forecasts <= bin_edges[i+1])\n",
    "\n",
    "        if np.sum(bin_mask) > 0:\n",
    "            observed_freq.append(np.mean(obs[bin_mask]))\n",
    "            forecast_prob.append(np.mean(prob_forecasts[bin_mask]))\n",
    "            bin_counts.append(np.sum(bin_mask))\n",
    "        else:\n",
    "            observed_freq.append(np.nan)\n",
    "            forecast_prob.append(np.mean([bin_edges[i], bin_edges[i+1]]))\n",
    "            bin_counts.append(0)\n",
    "\n",
    "    return np.array(observed_freq), np.array(forecast_prob), np.array(bin_counts)\n",
    "\n",
    "# Perform reliability analysis for one threshold\n",
    "thresh = 20\n",
    "obs_binary = (obs_temps > thresh).astype(int)\n",
    "prob_forecast = np.mean(ensemble_forecasts > thresh, axis=1)\n",
    "\n",
    "obs_freq, fore_prob, counts = reliability_analysis(obs_binary, prob_forecast)\n",
    "\n",
    "print(f\"\\nReliability Analysis (T > {thresh}°C):\")\n",
    "for i in range(len(obs_freq)):\n",
    "    if not np.isnan(obs_freq[i]):\n",
    "        print(f\"Forecast bin {i+1}: {fore_prob[i]:.2f} forecast, {obs_freq[i]:.2f} observed, {counts[i]} samples\")\n",
    "    else:\n",
    "        print(f\"Forecast bin {i+1}: {fore_prob[i]:.2f} forecast, no samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Verification Visualization\n",
    "\n",
    "Create visualizations to understand ensemble forecast performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble verification visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Ensemble vs observed scatter plot\n",
    "axes[0, 0].scatter(obs_temps[:500], ensemble_mean[:500], alpha=0.5, s=20)\n",
    "axes[0, 0].plot([obs_temps.min(), obs_temps.max()], [obs_temps.min(), obs_temps.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Observed Temperature (°C)')\n",
    "axes[0, 0].set_ylabel('Ensemble Mean Temperature (°C)')\n",
    "axes[0, 0].set_title(f'Ensemble Mean vs Observed (R² = {ms.R2(obs_temps, ensemble_mean):.3f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Single model vs observed scatter plot\n",
    "axes[0, 1].scatter(obs_temps[:500], mod_temps[:500], alpha=0.5, s=20)\n",
    "axes[0, 1].plot([obs_temps.min(), obs_temps.max()], [obs_temps.min(), obs_temps.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Observed Temperature (°C)')\n",
    "axes[0, 1].set_ylabel('Single Model Temperature (°C)')\n",
    "axes[0, 1].set_title(f'Single Model vs Observed (R² = {ms.R2(obs_temps, mod_temps):.3f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble spread vs absolute error\n",
    "abs_errors = np.abs(ensemble_mean - obs_temps)\n",
    "axes[0, 2].scatter(ensemble_std, abs_errors, alpha=0.5, s=20)\n",
    "axes[0, 2].set_xlabel('Ensemble Spread (°C)')\n",
    "axes[0, 2].set_ylabel('Absolute Error (°C)')\n",
    "axes[0, 2].set_title(f'Spread vs Error (Corr = {ms.pearson_correlation(ensemble_std, abs_errors):.3f})')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Time series with ensemble range\n",
    "n_plot = min(100, len(obs_temps))\n",
    "time_idx = range(n_plot)\n",
    "axes[1, 0].plot(time_idx, obs_temps[:n_plot], label='Observed', linewidth=2)\n",
    "axes[1, 0].plot(time_idx, ensemble_mean[:n_plot], label='Ensemble Mean', linewidth=2)\n",
    "axes[1, 0].fill_between(time_idx, ensemble_min[:n_plot], ensemble_max[:n_plot], alpha=0.3, label='Ensemble Range')\n",
    "axes[1, 0].set_xlabel('Time Index')\n",
    "axes[1, 0].set_ylabel('Temperature (°C)')\n",
    "axes[1, 0].set_title('Time Series with Ensemble Range')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Reliability diagram\n",
    "axes[1, 1].plot(fore_prob, obs_freq, 'o-', linewidth=2, markersize=8, label='Actual')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Perfect Reliability')\n",
    "axes[1, 1].set_xlabel('Forecast Probability')\n",
    "axes[1, 1].set_ylabel('Observed Frequency')\n",
    "axes[1, 1].set_title('Reliability Diagram (T > 20°C)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xlim(0, 1)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# Rank histogram (TBD: Simplified version)\n",
    "# Calculate ranks of observations relative to ensemble members\n",
    "n_rank_samples = min(1000, len(obs_temps))\n",
    "sample_indices = np.random.choice(len(obs_temps), n_rank_samples, replace=False)\n",
    "sample_obs = obs_temps[sample_indices]\n",
    "sample_ens = ensemble_forecasts[sample_indices, :]\n",
    "\n",
    "# Calculate ranks\n",
    "ranks = []\n",
    "for i in range(len(sample_obs)):\n",
    "    all_values = np.concatenate([[sample_obs[i]], sample_ens[i, :]])\n",
    "    sorted_indices = np.argsort(all_values)\n",
    "    obs_rank = np.where(sorted_indices == 0)[0][0] # 0 is the observation's position in sorted array\n",
    "    ranks.append(obs_rank)\n",
    "\n",
    "rank_counts, _ = np.histogram(ranks, bins=np.arange(0, n_members + 2))\n",
    "axes[1, 2].bar(range(len(rank_counts)), rank_counts, edgecolor='black', alpha=0.7)\n",
    "axes[1, 2].set_xlabel('Rank')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].set_title('Rank Histogram (Verification Rank)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Ensemble Analysis\n",
    "\n",
    "Simulate and analyze a multi-model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic multi-model ensemble\n",
    "n_models = 5  # Number of different models in the multi-model ensemble\n",
    "n_samples = len(obs_temps)\n",
    "\n",
    "# Generate forecasts from different models\n",
    "multi_model_forecasts = np.zeros((n_samples, n_models))\n",
    "model_names = [f'Model_{i+1}' for i in range(n_models)]\n",
    "\n",
    "for i in range(n_models):\n",
    "    # Each model has different characteristics\n",
    "    model_bias = np.random.normal(0, 1)  # Different bias for each model\n",
    "    model_corr = np.random.uniform(0.6, 0.95)  # Different correlation\n",
    "    model_noise = np.random.normal(0, np.random.uniform(0.5, 2.0), n_samples)  # Different noise level\n",
    "\n",
    "    # Create correlated forecast with observations\n",
    "    multi_model_forecasts[:, i] = (\n",
    "        obs_temps * model_corr +  # Correlated part\n",
    "        (1 - model_corr) * np.mean(obs_temps) +  # Climatological part\n",
    "        model_bias +  # Systematic bias\n",
    "        model_noise  # Random noise\n",
    "    )\n",
    "\n",
    "print(\"Multi-Model Ensemble Created:\")\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_rmse = ms.RMSE(obs_temps, multi_model_forecasts[:, i])\n",
    "    model_corr = ms.pearson_correlation(obs_temps, multi_model_forecasts[:, i])\n",
    "    print(f\"{model_name}: RMSE={model_rmse:.4f}, Correlation={model_corr:.4f}\")\n",
    "\n",
    "# Calculate multi-model ensemble statistics\n",
    "mm_mean = np.mean(multi_model_forecasts, axis=1)\n",
    "mm_std = np.std(multi_model_forecasts, axis=1)\n",
    "\n",
    "print(\"\\nMulti-Model Ensemble Performance:\")\n",
    "print(f\"Mean RMSE: {ms.RMSE(obs_temps, mm_mean):.4f}\")\n",
    "print(f\"Mean Correlation: {ms.pearson_correlation(obs_temps, mm_mean):.4f}\")\n",
    "\n",
    "# Compare single best model vs multi-model ensemble\n",
    "model_rmses = [ms.RMSE(obs_temps, multi_model_forecasts[:, i]) for i in range(n_models)]\n",
    "best_model_idx = np.argmin(model_rmses)\n",
    "best_model_rmse = model_rmses[best_model_idx]\n",
    "\n",
    "print(f\"\\nBest Single Model (Model_{best_model_idx+1}): RMSE={best_model_rmse:.4f}\")\n",
    "print(f\"Multi-Model Ensemble: RMSE={ms.RMSE(obs_temps, mm_mean):.4f}\")\n",
    "print(f\"Ensemble Improvement: {(best_model_rmse - ms.RMSE(obs_temps, mm_mean)) / best_model_rmse * 100:.2f}%\")\n",
    "\n",
    "# Calculate model weights based on performance\n",
    "model_weights = 1.0 / np.array(model_rmses)  # Inverse of RMSE as weight\n",
    "model_weights = model_weights / np.sum(model_weights)  # Normalize\n",
    "\n",
    "# Calculate weighted ensemble mean\n",
    "weighted_mean = np.zeros(n_samples)\n",
    "for i in range(n_models):\n",
    "    weighted_mean += model_weights[i] * multi_model_forecasts[:, i]\n",
    "\n",
    "print(f\"\\nWeighted Multi-Model Ensemble: RMSE={ms.RMSE(obs_temps, weighted_mean):.4f}\")\n",
    "print(f\"Weights: {model_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Ranking and Skill Assessment\n",
    "\n",
    "Rank ensemble members and assess their individual skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank ensemble members by performance\n",
    "member_metrics = []\n",
    "for i in range(n_members):\n",
    "    member_forecasts = ensemble_forecasts[:, i]\n",
    "    member_metrics.append({\n",
    "        'member': f'Member_{i+1}',\n",
    "        'RMSE': ms.RMSE(obs_temps, member_forecasts),\n",
    "        'MAE': ms.MAE(obs_temps, member_forecasts),\n",
    "        'Correlation': ms.pearson_correlation(obs_temps, member_forecasts),\n",
    "        'MBE': ms.MBE(obs_temps, member_forecasts)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and sort by RMSE\n",
    "member_df = pd.DataFrame(member_metrics)\n",
    "member_df = member_df.sort_values('RMSE')\n",
    "member_df['Rank'] = range(1, len(member_df) + 1)\n",
    "\n",
    "print(\"Ensemble Members Ranked by RMSE:\")\n",
    "print(member_df[['Rank', 'member', 'RMSE', 'Correlation', 'MBE']].round(4))\n",
    "\n",
    "# Calculate ensemble skill scores\n",
    "print(\"\\nEnsemble Skill Scores:\")\n",
    "\n",
    "# Calculate RMSE of ensemble mean vs climatology reference\n",
    "climatology = np.mean(obs_temps)\n",
    "rmse_ensemble = ms.RMSE(obs_temps, ensemble_mean)\n",
    "rmse_clim = ms.RMSE(obs_temps, np.full_like(obs_temps, climatology))\n",
    "\n",
    "ss_rmse = ms.SS(mse_model=rmse_ensemble**2, mse_reference=rmse_clim**2)\n",
    "print(f\"RMSE Skill Score: {ss_rmse:.4f}\")\n",
    "\n",
    "# Nash-Sutcliffe Efficiency\n",
    "nse = ms.NSE(obs_temps, ensemble_mean)\n",
    "print(f\"NSE: {nse:.4f}\")\n",
    "\n",
    "# Calculate ensemble spread-skill relationship\n",
    "abs_errors = np.abs(ensemble_mean - obs_temps)\n",
    "spread_skill_corr = ms.pearson_correlation(ensemble_std, abs_errors)\n",
    "print(f\"Spread-Skill Correlation: {spread_skill_corr:.4f}\")\n",
    "\n",
    "# Calculate ensemble reliability metrics\n",
    "reliability_metrics = []\n",
    "for thresh in [10, 20]:\n",
    "    obs_binary = (obs_temps > thresh).astype(int)\n",
    "    prob_forecast = np.mean(ensemble_forecasts > thresh, axis=1)\n",
    "    bs = ms.BS(obs_binary, prob_forecast)\n",
    "    reliability_metrics.append({'threshold': f'T>{thresh}°C', 'Brier_Score': bs})\n",
    "\n",
    "print(\"\\nReliability Metrics:\")\n",
    "for rm in reliability_metrics:\n",
    "    print(f\"{rm['threshold']}: Brier Score = {rm['Brier_Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Forecast Evaluation Dashboard\n",
    "\n",
    "Create a comprehensive dashboard for ensemble evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble evaluation dashboard\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# Ensemble member performance comparison\n",
    "rmse_values = [ms.RMSE(obs_temps, ensemble_forecasts[:, i]) for i in range(n_members)]\n",
    "axes[0, 0].bar(range(1, n_members + 1), rmse_values)\n",
    "axes[0, 0].set_xlabel('Ensemble Member')\n",
    "axes[0, 0].set_ylabel('RMSE')\n",
    "axes[0, 0].set_title('RMSE by Ensemble Member')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Multi-model comparison\n",
    "mm_rmses = [ms.RMSE(obs_temps, multi_model_forecasts[:, i]) for i in range(n_models)]\n",
    "all_rmses = rmse_values + mm_rmses\n",
    "labels = [f'E{i+1}' for i in range(n_members)] + [f'MM{i+1}' for i in range(n_models)]\n",
    "colors = ['blue'] * n_members + ['red'] * n_models\n",
    "\n",
    "axes[0, 1].bar(range(len(all_rmses)), all_rmses, color=colors)\n",
    "axes[0, 1].set_xlabel('Model/Member')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('RMSE Comparison (Ensemble vs Multi-Model)')\n",
    "axes[0, 1].set_xticks(range(len(labels)))\n",
    "axes[0, 1].set_xticklabels(labels, rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble spread histogram\n",
    "axes[0, 2].hist(ensemble_std, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 2].set_xlabel('Ensemble Spread')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title(f'Ensemble Spread Distribution (Mean: {np.mean(ensemble_std):.3f})')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Probability vs observed for threshold\n",
    "thresh = 15\n",
    "obs_binary = (obs_temps > thresh).astype(int)\n",
    "prob_forecast = np.mean(ensemble_forecasts > thresh, axis=1)\n",
    "\n",
    "axes[1, 0].scatter(prob_forecast, obs_binary, alpha=0.5, s=20)\n",
    "axes[1, 0].set_xlabel('Forecast Probability (T > 15°C)')\n",
    "axes[1, 0].set_ylabel('Observed (1) or Not (0)')\n",
    "axes[1, 0].set_title('Probability vs Observed (T > 15°C)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble forecast uncertainty vs performance\n",
    "axes[1, 1].scatter(ensemble_std, abs_errors, alpha=0.5, s=20)\n",
    "axes[1, 1].set_xlabel('Ensemble Uncertainty (Std)')\n",
    "axes[1, 1].set_ylabel('Absolute Error')\n",
    "axes[1, 1].set_title(f'Uncertainty vs Performance (Corr: {spread_skill_corr:.3f})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC curve for one threshold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(obs_binary, prob_forecast)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1, 2].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "axes[1, 2].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[1, 2].set_xlabel('False Positive Rate')\n",
    "axes[1, 2].set_ylabel('True Positive Rate')\n",
    "axes[1, 2].set_title('ROC Curve (T > 15°C)')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Ensemble forecast distribution for a single time point\n",
    "time_idx = 50\n",
    "axes[2, 0].hist(ensemble_forecasts[time_idx, :], bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[2, 0].axvline(obs_temps[time_idx], color='red', linewidth=2, label=f'Observed: {obs_temps[time_idx]:.2f}')\n",
    "axes[2, 0].axvline(ensemble_mean[time_idx], color='blue', linewidth=2, label=f'Ensemble Mean: {ensemble_mean[time_idx]:.2f}')\n",
    "axes[2, 0].set_xlabel('Temperature (°C)')\n",
    "axes[2, 0].set_ylabel('Count')\n",
    "axes[2, 0].set_title(f'Ensemble Distribution at Time {time_idx}')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Ranked probability score components\n",
    "axes[2, 1].plot(range(1, n_members + 1), rmse_values, 'o-', linewidth=2, markersize=8)\n",
    "axes[2, 1].set_xlabel('Ensemble Member (Ranked)')\n",
    "axes[2, 1].set_ylabel('RMSE')\n",
    "axes[2, 1].set_title('Ensemble Member Performance')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Multi-model performance\n",
    "axes[2, 2].bar(range(1, n_models + 1), mm_rmses, alpha=0.7)\n",
    "axes[2, 2].set_xlabel('Multi-Model')\n",
    "axes[2, 2].set_ylabel('RMSE')\n",
    "axes[2, 2].set_title('Multi-Model Performance')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated ensemble analysis and multi-model evaluation techniques:\n",
    "\n",
    "1. **Ensemble Statistics**: Mean, spread, min/max, and performance metrics\n",
    "2. **Probabilistic Verification**: Brier Score, reliability analysis, rank histograms\n",
    "3. **Multi-Model Ensembles**: Combining forecasts from different models\n",
    "4. **Ensemble Ranking**: Assessing individual member performance\n",
    "5. **Skill Assessment**: Ensemble skill scores and spread-skill relationships\n",
    "6. **Comprehensive Dashboard**: Multi-faceted ensemble evaluation\n",
    "\n",
    "These methods provide a complete framework for evaluating ensemble and multi-model forecasting systems in atmospheric sciences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}