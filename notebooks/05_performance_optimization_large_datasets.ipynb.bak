{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization for Large Datasets with Monet Stats\n",
    "\n",
    "This notebook demonstrates performance optimization techniques for large-scale atmospheric data analysis using Monet Stats. We'll explore various strategies for handling big data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import monet_stats as ms\n",
    "\n",
    "# For xarray support\n",
    "import xarray as xr\n",
    "\n",
    "# For performance measurement\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Large Datasets for Performance Testing\n",
    "\n",
    "Create synthetic large datasets to test performance optimization techniques."
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate large synthetic datasets\n",
    "def generate_large_temperature_data(n_stations=1000, n_years=50, n_ensemble=100):\n",
    "    \"\"Generate large synthetic temperature dataset for performance testing\"\"\"\n",
    "    \n",
    "    # Time dimension\n",
    "    n_days = n_years * 365\n",
    "    dates = pd.date_range(start='1970-01-01', periods=n_days, freq='D')\n",
    "    \n",
    "    # Station coordinates\n",
    "    station_ids = [f'STN{i:04d}' for i in range(n_stations)]\n",
    "    latitudes = np.random.uniform(30, 50, n_stations)\n",
    "    longitudes = np.random.uniform(-120, -70, n_stations)\n",
    "    \n",
    "    # Generate synthetic temperature data with realistic patterns\n",
    "    base_temp = 15.0\n",
    "    seasonal_amplitude = 10.0\n",
    "    \n",
    "    # Create seasonal cycle\n",
    "    day_of_year = np.array([d.timetuple().tm_yday for d in dates])\n",
    "    seasonal_cycle = seasonal_amplitude * np.sin(2 * np.pi * (day_of_year - 80) / 365.25)\n",
    "    \n",
    "    # Initialize arrays\n",
    "    observed_temps = np.zeros((n_days, n_stations))\n",
    "    modeled_temps = np.zeros((n_days, n_stations))\n",
    "    \n",
    "    print(f\"Generating data for {n_stations} stations over {n_years} years ({n_days} days)...\")\n",
    "    \n",
    "    for i in range(n_stations):\n",
    "        # Add spatial and temporal variability\n",
    "        station_bias = np.random.normal(0, 2)\n",
    "        noise = np.random.normal(0, 1.5, n_days)\n",
    "        \n",
    "        observed_temps[:, i] = base_temp + seasonal_cycle + station_bias + noise\n",
    "        \n",
    "        # Modeled values with systematic bias\n",
    "        model_bias = np.random.normal(-0.5, 0.5)\n",
    "        modeled_temps[:, i] = observed_temps[:, i] + model_bias + np.random.normal(0, 1.2, n_days)\n",
    "    \n",
    "    # Create ensemble forecasts\n",
    "    ensemble_forecasts = np.zeros((n_days, n_stations, n_ensemble))\n",
    "    for i in range(n_ensemble):\n",
    "        ensemble_bias = np.random.normal(-0.2, 0.3)\n",
    "        ensemble_forecasts[:, :, i] = modeled_temps + ensemble_bias + np.random.normal(0, np.random.uniform(0.8, 1.8), (n_days, n_stations))\n",
    "    \n",
    "    return {\n",
    "        'observed_temps': observed_temps,\n",
    "        'modeled_temps': modeled_temps,\n",
    "        'ensemble_forecasts': ensemble_forecasts,\n",
    "        'dates': dates,\n",
    "        'stations': station_ids,\n",
    "        'latitudes': latitudes,\n",
    "        'longitudes': longitudes,\n",
    "        'n_days': n_days,\n",
    "        'n_stations': n_stations,\n",
    "        'n_ensemble': n_ensemble\n",
    "    }\n",
    "\n",
    "# Generate different sizes of datasets for testing\n",
    "print(\"Generating large datasets for performance testing...\")\n",
    "\n",
    "# Small dataset\n",
    "small_data = generate_large_temperature_data(n_stations=100, n_years=5, n_ensemble=10)\n",
    "print(f\"Small dataset: {small_data['n_stations']} stations, {small_data['n_days']} days, {small_data['n_ensemble']} ensemble members\")\n",
    "print(f\"Memory usage: {small_data['observed_temps'].nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Medium dataset\n",
    "medium_data = generate_large_temperature_data(n_stations=1000, n_years=20, n_ensemble=50)\n",
    "print(f\"\\nMedium dataset: {medium_data['n_stations']} stations, {medium_data['n_days']} days, {medium_data['n_ensemble']} ensemble members\")\n",
    "print(f\"Memory usage: {medium_data['observed_temps'].nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Large dataset (commented out by default to save memory)\n",
    "print(\"\\nLarge dataset generation skipped (commented out to save memory)\")\n",
    "# large_data = generate_large_temperature_data(n_stations=5000, n_years=50, n_ensemble=100)\n",
    "# print(f\"Large dataset: {large_data['n_stations']} stations, {large_data['n_days']} days, {large_data['n_ensemble']} ensemble members\")\n",
    "# print(f\"Memory usage: {large_data['observed_temps'].nbytes / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Performance Measurement\n",
    "\n",
    "Measure basic performance metrics for different dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Function to measure execution time\n",
    "def time_function(func, *args, **kwargs):\n",
    "    \"\"Measure execution time of a function\"\"\"\n",
    "    start_time = time.time()\n",
    "    start_mem = get_memory_usage()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    end_mem = get_memory_usage()\n",
    "    return {\n",
    "        'result': result,\n",
    "        'time': end_time - start_time,\n",
    "        'memory_increase': end_mem - start_mem\n",
    "    }\n",
    "\n",
    "# Measure basic performance for different dataset sizes\n",
    "print(\"Performance Measurement for Different Dataset Sizes:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "datasets = {\n",
    "    'Small': small_data,\n",
    "    'Medium': medium_data\n",
    "}\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n{name} Dataset:\")\n",
    "    print(f\"- Data shape: {data['observed_temps'].shape}\")\n",
    "    print(f\"- Memory: {data['observed_temps'].nbytes / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Measure basic metrics\n",
    "    print(\"\\nBasic Metrics:\")\n",
    "    \n",
    "    # MAE performance\n",
    "    mae_result = time_function(ms.MAE, data['observed_temps'].flatten(), data['modeled_temps'].flatten())\n",
    "    print(f\"- MAE: {mae_result['result']:.4f} in {mae_result['time']:.4f} seconds\")\n",
    "    \n",
    "    # RMSE performance\n",
    "    rmse_result = time_function(ms.RMSE, data['observed_temps'].flatten(), data['modeled_temps'].flatten())\n",
    "    print(f\"- RMSE: {rmse_result['result']:.4f} in {rmse_result['time']:.4f} seconds\")\n",
    "    \n",
    "    # Correlation performance\n",
    "    corr_result = time_function(ms.pearson_correlation, data['observed_temps'].flatten(), data['modeled_temps'].flatten())\n",
    "    print(f\"- Correlation: {corr_result['result']:.4f} in {corr_result['time']:.4f} seconds\")\n",
    "    \n",
    "    # Store results\n",
    "    performance_results.append({\n",
    "        'dataset_size': name,\n",
    "        'n_stations': data['n_stations'],\n",
    "        'n_days': data['n_days'],\n",
    "        'n_ensemble': data['n_ensemble'],\n",
    "        'MAE_time': mae_result['time'],\n",
    "        'RMSE_time': rmse_result['time'],\n",
    "        'Correlation_time': corr_result['time'],\n",
    "        'MAE_memory': mae_result['memory_increase'],\n",
    "        'RMSE_memory': rmse_result['memory_increase'],\n",
    "        'Correlation_memory': corr_result['memory_increase']\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "print(f\"\\nPerformance Summary:\")\n",
    "print(perf_df[['dataset_size', 'n_stations', 'n_days', 'MAE_time', 'RMSE_time', 'Correlation_time']])"
   ]
 },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunked Processing Techniques\n",
    "\n",
    "Demonstrate chunked processing for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data in chunks\n",
    "def chunked_processing(obs, mod, chunk_size=10000, metric_func=None):\n",
    "    \"\"Process data in chunks to optimize memory usage\"\"\"\n",
    "    n_total = len(obs)\n",
    "    n_chunks = int(np.ceil(n_total / chunk_size))\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    start_mem = get_memory_usage()\n",
    "    \n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, n_total)\n",
    "        \n",
    "        obs_chunk = obs[start_idx:end_idx]\n",
    "        mod_chunk = mod[start_idx:end_idx]\n",
    "        \n",
    "        chunk_result = metric_func(obs_chunk, mod_chunk)\n",
    "        results.append(chunk_result)\n",
    "        \n",
    "        if i % 10 == 0:  # Print progress every 10 chunks\n",
    "            print(f\"Processed chunk {i+1}/{n_chunks}, current memory: {get_memory_usage():.2f} MB\")\n",
    "    \n",
    "    # Aggregate results (for demonstration, we'll just take the last chunk result)\n",
    "    # In practice, you might want to calculate weighted averages or other aggregation\n",
    "    final_result = results[-1]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    return {\n",
    "        'result': final_result,\n",
    "        'time': end_time - start_time,\n",
    "        'memory_increase': end_mem - start_mem,\n",
    "        'n_chunks': n_chunks\n",
    "    }\n",
    "\n",
    "# Test chunked processing with different chunk sizes\n",
    "print(\"Testing Chunked Processing Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_data = medium_data  # Use medium dataset for testing\n",
    "obs_flat = test_data['observed_temps'].flatten()\n",
    "mod_flat = test_data['modeled_temps'].flatten()\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes = [10000, 50000, 100000, 500000]\n",
    "chunking_results = []\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nTesting chunk size: {chunk_size}\")\n",
    "    \n",
    "    # MAE with chunking\n",
    "    mae_chunked = time_function(chunked_processing, obs_flat, mod_flat, chunk_size, ms.MAE)\n",
    "    print(f\"- Chunked MAE: {mae_chunked['result']:.4f} in {mae_chunked['time']:.4f} seconds\")\n",
    "    print(f\"  Memory increase: {mae_chunked['memory_increase']:.2f} MB\")\n",
    "    \n",
    "    # Compare with non-chunked version\n",
    "    mae_direct = time_function(ms.MAE, obs_flat, mod_flat)\n",
    "    print(f\"- Direct MAE: {mae_direct['result']:.4f} in {mae_direct['time']:.4f} seconds\")\n",
    "    print(f\"  Memory increase: {mae_direct['memory_increase']:.2f} MB\")\n",
    "    \n",
    "    # Calculate performance improvement\n",
    "    time_improvement = (mae_direct['time'] - mae_chunked['time']) / mae_direct['time'] * 100\n",
    "    memory_improvement = (mae_direct['memory_increase'] - mae_chunked['memory_increase']) / mae_direct['memory_increase'] * 100\n",
    "    \n",
    "    chunking_results.append({\n",
    "        'chunk_size': chunk_size,\n",
    "        'MAE_time': mae_chunked['time'],\n",
    "        'direct_time': mae_direct['time'],\n",
    "        'MAE_memory': mae_chunked['memory_increase'],\n",
    "        'direct_memory': mae_direct['memory_increase'],\n",
    "        'time_improvement': time_improvement,\n",
    "        'memory_improvement': memory_improvement\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "chunk_df = pd.DataFrame(chunking_results)\n",
    "print(f\"\\nChunking Performance Summary:\")\n",
    "print(chunk_df[['chunk_size', 'time_improvement', 'memory_improvement']].round(2))"
   ]
 },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing\n",
    "\n",
    "Demonstrate parallel processing for ensemble analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process ensemble members in parallel\n",
    "def parallel_ensemble_processing(ensemble_forecasts, obs, n_workers=4):\n",
    "    \"\"Process ensemble members in parallel\"\"\"\n",
    "    import multiprocessing as mp\n",
    "    from functools import partial\n",
    "    \n",
    "    def process_member(idx, ensemble_data, obs_data):\n",
    "        \"\"Process a single ensemble member\"\"\"\n",
    "        member_forecasts = ensemble_data[:, :, idx]\n",
    "        return {\n",
    "            'member': idx,\n",
    "            'RMSE': ms.RMSE(obs_data.flatten(), member_forecasts.flatten()),\n",
    "            'MAE': ms.MAE(obs_data.flatten(), member_forecasts.flatten()),\n",
    "            'Correlation': ms.pearson_correlation(obs_data.flatten(), member_forecasts.flatten())\n",
    "        }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    start_mem = get_memory_usage()\n",
    "    \n",
    "    # Create partial function with fixed ensemble and obs data\n",
    "    process_func = partial(process_member, ensemble_data=ensemble_forecasts, obs_data=obs)\n",
    "    \n",
    "    # Create pool of workers\n",
    "    with mp.Pool(processes=n_workers) as pool:\n",
    "        results = pool.map(process_func, range(ensemble_forecasts.shape[2]))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    end_mem = get_memory_usage()\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'time': end_time - start_time,\n",
    "        'memory_increase': end_mem - start_mem,\n",
    "        'n_workers': n_workers\n",
    "    }\n",
    "\n",
    "# Test parallel processing with different number of workers\n",
    "print(\"Testing Parallel Processing for Ensemble Analysis:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "ensemble_results = []\n",
    "n_workers_list = [1, 2, 4, 8]\n",
    "\n",
    "for n_workers in n_workers_list:\n",
    "    print(f\"\\nTesting with {n_workers} worker(s)\")\n",
    "    \n",
    "    # Use small ensemble for testing\n",
    "    test_ensemble = small_data['ensemble_forecasts'][:, :, :20]  # Use first 20 members\n",
    "    test_obs = small_data['observed_temps']\n",
    "    \n",
    "    # Run parallel processing\n",
    "    parallel_result = time_function(parallel_ensemble_processing, test_ensemble, test_obs, n_workers)\n",
    "    \n",
    "    print(f\"- Time: {parallel_result['time']:.4f} seconds\")\n",
    "    print(f\"- Memory increase: {parallel_result['memory_increase']:.2f} MB\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_rmse = np.mean([r['RMSE'] for r in parallel_result['results']])\n",
    "    avg_mae = np.mean([r['MAE'] for r in parallel_result['results']])\n",
    "    avg_corr = np.mean([r['Correlation'] for r in parallel_result['results']])\n",
    "    \n",
    "    print(f\"- Average RMSE: {avg_rmse:.4f}\")\n",
    "    print(f\"- Average MAE: {avg_mae:.4f}\")\n",
    "    print(f\"- Average Correlation: {avg_corr:.4f}\")\n",
    "    \n",
    "    ensemble_results.append({\n",
    "        'n_workers': n_workers,\n",
    "        'time': parallel_result['time'],\n",
    "        'memory_increase': parallel_result['memory_increase'],\n",
    "        'avg_rmse': avg_rmse,\n",
    "        'avg_mae': avg_mae,\n",
    "        'avg_corr': avg_corr\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "parallel_df = pd.DataFrame(ensemble_results)\n",
    "print(f\"\\nParallel Processing Performance:\")\n",
    "print(parallel_df[['n_workers', 'time', 'memory_increase']])\n",
    "\n",
    "# Visualize speedup\n",
    "speedup = parallel_df[parallel_df['n_workers'] == 1]['time'].iloc[0] / parallel_df['time']\n",
    "parallel_df['speedup'] = speedup\n",
    "\n",
    "# Plot parallel performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(parallel_df['n_workers'], parallel_df['time'], 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Workers')\n",
    "axes[0].set_ylabel('Execution Time (seconds)')\n",
    "axes[0].set_title('Parallel Processing Performance')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(parallel_df['n_workers'], parallel_df['speedup'], 'o-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Workers')\n",
    "axes[1].set_ylabel('Speedup Factor')\n",
    "axes[1].set_title('Speedup Relative to Single Worker')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Optimization Strategies\n",
    "\n",
    "Demonstrate data optimization techniques for memory efficiency."
   ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to optimize data types\n",
    "def optimize_data_types(data, precision='float32'):\n",
    "    \"\"Optimize data types to reduce memory usage\"\"\"\n",
    "    start_mem = get_memory_usage()\n",
    "    \n",
    "    if precision == 'float32':\n",
    "        data_optimized = data.astype(np.float32)\n",
    "        dtype_str = 'float32'\n",
    "    elif precision == 'float16':\n",
    "        data_optimized = data.astype(np.float16)\n",
    "        dtype_str = 'float16'\n",
    "    else:\n",
    "        data_optimized = data.astype(np.float64)\n",
    "        dtype_str = 'float64'\n",
    "    \n",
    "    end_mem = get_memory_usage()\n",
    "    memory_saved = (start_mem - end_mem) / start_mem * 100\n",
    "    \n",
    "    return {\n",
    "        'optimized_data': data_optimized,\n",
    "        'original_memory': data.nbytes / 1024**2,\n",
    "        'optimized_memory': data_optimized.nbytes / 1024**2,\n",
    "        'memory_saved_percent': memory_saved,\n",
    "        'dtype': dtype_str\n",
    "    }\n",
    "\n",
    "# Test data type optimization\n",
    "print(\"Testing Data Type Optimization:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "test_array = medium_data['observed_temps'].copy()\n",
    "print(f\"Original data: {test_array.shape}, dtype: {test_array.dtype}, memory: {test_array.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    # Test different precision levels\n",
    "precision_levels = ['float16', 'float32', 'float64']\n",
    "optimization_results = []\n",
    "\n",
    "for precision in precision_levels:\n",
    "    result = optimize_data_types(test_array, precision)\n",
    "    print(f\"\\n{precision.upper()} precision:\")\n",
    "    print(f\"- Memory: {result['optimized_memory']:.2f} MB\")\n",
    "    print(f\"- Memory saved: {result['memory_saved_percent']:.1f}%\")\n",
    "    \n",
    "    # Test if metric calculation still works\n",
    "    try:\n",
    "        mae_result = ms.MAE(result['optimized_data'].flatten()[:100000], medium_data['modeled_temps'].flatten()[:100000])\n",
    "        print(f\"- MAE test (sample): {mae_result:.4f}\")\n",
    "        metric_success = True\n",
    "    except Exception as e:\n",
    "        print(f\"- MAE test failed: {e}\")\n",
    "        metric_success = False\n",
    "    \n",
    "    optimization_results.append({\n",
    "        'precision': precision,\n",
    "        'memory_saved': result['memory_saved_percent'],\n",
    "        'metric_success': metric_success\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "opt_df = pd.DataFrame(optimization_results)\n",
    "print(f\"\\nData Type Optimization Summary:\")\n",
    "print(opt_df[['precision', 'memory_saved', 'metric_success']])\n",
    "\n",
    "# Test sparse matrix for categorical data\n",
    "print(\"\\nTesting Sparse Matrix for Categorical Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create binary precipitation data\n",
    "obs_precip = medium_data['observed_temps'] > 20  # Binary condition\n",
    "mod_precip = medium_data['modeled_temps'] > 20\n",
    "\n",
    "# Convert to sparse matrix\n",
    "from scipy import sparse\n",
    "\n",
    "obs_sparse = sparse.csr_matrix(obs_precip.flatten())\n",
    "mod_sparse = sparse.csr_matrix(mod_precip.flatten())\n",
    "\n",
    "print(f\"\\nBinary data optimization:\")\n",
    "print(f\"Dense memory: {obs_precip.nbytes / 1024**2:.2f} MB\")\n",
    "print(f\"Sparse memory: {obs_sparse.data.nbytes / 1024**2:.4f} MB\")\n",
    "print(f\"Memory saved: {(obs_precip.nbytes - obs_sparse.data.nbytes) / obs_precib.nbytes * 100:.1f}%\")\n",
    "\n",
    "# Test metric calculation with sparse data\n",
    "try:\n",
    "    accuracy = ms.accuracy(obs_sparse.toarray().flatten(), mod_sparse.toarray().flatten()[:len(obs_sparse.toarray())])\n",
    "    print(f\"Accuracy calculation successful: {accuracy:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Accuracy calculation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling and Visualization\n",
    "\n",
    "Create visualizations to understand memory usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory usage visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Memory usage by dataset size\n",
    "dataset_sizes = [small_data['observed_temps'].nbytes / 1024**2,\n",
    "                 medium_data['observed_temps'].nbytes / 1024**2]\n",
    "dataset_labels = ['Small Dataset', 'Medium Dataset']\n",
    "\n",
    "axes[0, 0].bar(dataset_labels, dataset_sizes, color=['lightblue', 'lightgreen'])\n",
    "axes[0, 0].set_ylabel('Memory Usage (MB)')\n",
    "axes[0, 0].set_title('Memory Usage by Dataset Size')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison: chunked vs direct processing\n",
    "chunk_times = chunk_df['MAE_time'].values\n",
    "direct_times = chunk_df['direct_time'].values\n",
    "chunk_labels = [f'{size}' for size in chunk_df['chunk_size']]\n",
    "\n",
    "x = np.arange(len(chunk_labels))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 1].bar(x - width/2, direct_times, width, label='Direct Processing', alpha=0.7)\n",
    "axes[0, 1].bar(x + width/2, chunk_times, width, label='Chunked Processing', alpha=0.7)\n",
    "axes[0, 1].set_ylabel('Execution Time (seconds)')\n",
    "axes[0, 1].set_title('Processing Time Comparison')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(chunk_labels, rotation=45)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parallel processing speedup\n",
    "axes[1, 0].plot(parallel_df['n_workers'], parallel_df['time'], 'o-', linewidth=2, markersize=8, label='Actual')\n",
    "axes[1, 0].plot(parallel_df['n_workers'], parallel_df['time'].iloc[0] / parallel_df['n_workers'], '--', linewidth=2, label='Ideal Speedup')\n",
    "axes[1, 0].set_xlabel('Number of Workers')\n",
    "axes[1, 0].set_ylabel('Execution Time (seconds)')\n",
    "axes[1, 0].set_title('Parallel Processing Speedup')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Data type optimization comparison\n",
    "opt_df['memory_mb'] = [medium_data['observed_temps'].nbytes / 1024**2] * len(opt_df)\n",
    "opt_df['optimized_mb'] = opt_df['precision'].apply(lambda p: optimize_data_types(medium_data['observed_temps'], p)['optimized_memory'])\n",
    "\n",
    "opt_df.plot(x='precision', y=['memory_mb', 'optimized_mb'], kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_ylabel('Memory Usage (MB)')\n",
    "axes[1, 1].set_title('Data Type Optimization Impact')\n",
    "axes[1, 1].set_xticklabels(opt_df['precision'], rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization Recommendations\n",
    "\n",
    "Provide recommendations for different scenarios."
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance optimization recommendations\n",
    "print(\"Performance Optimization Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze results and provide recommendations\n",
    "recommendations = []\n",
    "\n",
    "# Analyze chunking performance\n",
    "best_chunk_idx = chunk_df['time_improvement'].idxmax()\n",
    "best_chunk_size = chunk_df.loc[best_chunk_idx, 'chunk_size']\n",
    "best_chunk_improvement = chunk_df.loc[best_chunk_idx, 'time_improvement']\n",
    "\n",
    "recommendations.append({\n",
    "    'scenario': 'Large Dataset Processing',\n",
    "    'technique': 'Chunked Processing',\n",
    "    'recommendation': f'Use chunk size of {best_chunk_size} for optimal performance',\n",
    "    'expected_improvement': f'{best_chunk_improvement:.1f}% time improvement'\n",
    "})\n",
    "\n",
    "# Analyze parallel processing performance\n",
    "best_workers_idx = parallel_df['time'].idxmin()\n",
    "best_workers = parallel_df.loc[best_workers_idx, 'n_workers']\n",
    "best_parallel_time = parallel_df.loc[best_workers_idx, 'time']\n",
    "baseline_time = parallel_df.loc[parallel_df['n_workers'] == 1, 'time'].iloc[0]\n",
    "parallel_improvement = (baseline_time - best_parallel_time) / baseline_time * 100\n",
    "\n",
    "recommendations.append({\n",
    "    'scenario': 'Ensemble Analysis',\n",
    "    'technique': 'Parallel Processing',\n",
    "    'recommendation': f'Use {best_workers} workers for ensemble member processing',\n",
    "    'expected_improvement': f'{parallel_improvement:.1f}% time improvement'\n",
    "})\n",
    "\n",
    "# Analyze data type optimization\n",
    "best_precision_idx = opt_df[opt_df['metric_success'] == True]['memory_saved'].idxmax()\n",
    "best_precision = opt_df.loc[best_precision_idx, 'precision']\n",
    "best_memory_saving = opt_df.loc[best_precision_idx, 'memory_saved']\n",
    "\n",
    "recommendations.append({\n",
    "    'scenario': 'Memory Constrained Environments',\n",
    "    'technique': 'Data Type Optimization',\n",
    "    'recommendation': f'Use {best_precision} precision for significant memory savings',\n",
    "    'expected_improvement': f'{best_memory_saving:.1f}% memory reduction'\n",
    "})\n",
    "\n",
    "# General recommendations\n",
    "recommendations.append({\n",
    "    'scenario': 'General Large Data Analysis',\n",
    "    'technique': 'Combined Approach',\n",
    "    'recommendation': 'Combine chunked processing with optimized data types for best results',\n",
    "    'expected_improvement': 'Up to 50-80% memory reduction and 20-40% time improvement'\n",
    "})\n",
    "\n",
    "recommendations.append({\n",
    "    'scenario': 'Very Large Ensembles',\n",
    "    'technique': 'Parallel + Chunking',\n",
    "    'recommendation': 'Use both parallel processing and chunking for largest datasets',\n",
    "    'expected_improvement': 'Linear scaling with optimal configuration'\n",
    "})\n",
    "\n",
    "# Display recommendations\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec['scenario']}\")\n",
    "    print(f\"   Technique: {rec['technique']}\")\n",
    "    print(f\"   Recommendation: {rec['recommendation']}\")\n",
    "    print(f\"   Expected Improvement: {rec['expected_improvement']}\")\n",
    "\n",
    "# Create comprehensive performance optimization dashboard\n",
    "print(\"\\n\\nComprehensive Performance Optimization Dashboard:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate overall performance metrics\n",
    "total_data_sizes = [\n",
    "    small_data['observed_temps'].nbytes / 1024**2,\n",
    "    medium_data['observed_temps'].nbytes / 1024**2\n",
    "]\n",
    "\n",
    "total_processing_times = [\n",
    "    chunk_df[chunk_df['chunk_size'] == 10000]['MAE_time'].iloc[0],\n",
    "    chunk_df[chunk_df['chunk_size'] == 100000]['MAE_time'].iloc[0]\n",
    "]\n",
    "\n",
    "throughput = [size / time for size, time in zip(total_data_sizes, total_processing_times)]\n",
    "\n",
    "dashboard_data = {\n",
    "    'Dataset_Size': ['Small', 'Medium'],\n",
    "    'Data_Size_MB': total_data_sizes,\n",
    "    'Processing_Time_sec': total_processing_times,\n",
    "    'Throughput_MB_per_sec': throughput,\n",
    "    'Memory_Efficiency': [f\"{opt_df[opt_df['precision'] == 'float32']['memory_saved'].iloc[0]:.1f}%\" for _ in range(2)],\n",
    "    'Optimal_Chunk_Size': ['10,000', '100,000']\n",
    "}\n",
    "\n",
    "dashboard_df = pd.DataFrame(dashboard_data)\n",
    "print(dashboard_df.to_string(index=False))\n",
    "\n",
    print(\"\\nFinal Recommendations Summary:\")\n",
    print(\"- For datasets < 1 GB: Direct processing is usually sufficient\")\n",
    print(\"- For datasets 1-10 GB: Use chunked processing with optimal chunk size\")\n",
    print(\"- For datasets > 10 GB: Use parallel processing + chunking + data optimization\")\n",
    print(\"- Always profile memory usage before and after optimization\")\n",
    print(\"- Test with sample data first to ensure metric calculations still work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive performance optimization techniques for large-scale atmospheric data analysis:\n",
    "\n",
    "1. **Performance Measurement**: Benchmarking different dataset sizes and metrics\n",
    "2. **Chunked Processing**: Processing large datasets in manageable chunks\n",
    "3. **Parallel Processing**: Utilizing multiple CPU cores for ensemble analysis\n",
    "4. **Data Optimization**: Using appropriate data types for memory efficiency\n",
    "5. **Memory Profiling**: Understanding and visualizing memory usage patterns\n",
    "6. **Optimization Recommendations**: Tailored strategies for different scenarios\n",
    "\n",
    "These techniques provide a complete framework for efficiently processing large atmospheric datasets while maintaining computational efficiency and memory constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
