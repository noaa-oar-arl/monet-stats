{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Observation Comparison with Monet Stats\n",
    "\n",
    "This notebook demonstrates comprehensive model-observation comparison workflows using Monet Stats. We'll explore various atmospheric variables and evaluation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# For xarray support\n",
    "import monet_stats as ms\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Multiple Example Datasets\n",
    "\n",
    "We'll use temperature, precipitation, and wind datasets for comprehensive comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example datasets\n",
    "temp_df = pd.read_csv('../data/temperature_obs_mod.csv')\n",
    "precip_df = pd.read_csv('../data/precipitation_obs_mod.csv')\n",
    "wind_df = pd.read_csv('../data/wind_obs_mod.csv')\n",
    "\n",
    "print(\"Temperature dataset shape:\", temp_df.shape)\n",
    "print(\"Precipitation dataset shape:\", precip_df.shape)\n",
    "print(\"Wind dataset shape:\", wind_df.shape)\n",
    "\n",
    "# Display basic info for each dataset\n",
    "print(\"\\nTemperature dataset:\")\n",
    "print(temp_df[['observed_temp', 'modeled_temp']].describe())\n",
    "\n",
    "print(\"\\nPrecipitation dataset:\")\n",
    "print(precip_df[['observed_precip', 'modeled_precip']].describe())\n",
    "\n",
    "print(\"\\nWind dataset:\")\n",
    "print(wind_df[['observed_wind_speed', 'modeled_wind_speed']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Model-Observation Comparison\n",
    "\n",
    "Analyze temperature forecast performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temperature data\n",
    "obs_temp = temp_df['observed_temp'].values\n",
    "mod_temp = temp_df['modeled_temp'].values\n",
    "\n",
    "# Calculate comprehensive temperature metrics\n",
    "temp_metrics = {\n",
    "    'MAE': ms.MAE(obs_temp, mod_temp),\n",
    "    'RMSE': ms.RMSE(obs_temp, mod_temp),\n",
    "    'MBE': ms.MBE(obs_temp, mod_temp),\n",
    "    'Correlation': ms.pearson_correlation(obs_temp, mod_temp),\n",
    "    'R2': ms.R2(obs_temp, mod_temp),\n",
    "    'NSE': ms.NSE(obs_temp, mod_temp),\n",
    "    'mNSE': ms.mNSE(obs_temp, mod_temp),\n",
    "    'rNSE': ms.rNSE(obs_temp, mod_temp)\n",
    "}\n",
    "\n",
    "print(\"Temperature Model Performance Metrics:\")\n",
    "for metric, value in temp_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Calculate bias by temperature range\n",
    "temp_ranges = [\n",
    "    (\"Very Cold (< 0°C)\", obs_temp < 0),\n",
    "    (\"Cold (0-10°C)\", (obs_temp >= 0) & (obs_temp < 10)),\n",
    "    (\"Mild (10-20°C)\", (obs_temp >= 10) & (obs_temp < 20)),\n",
    "    (\"Warm (20-30°C)\", (obs_temp >= 20) & (obs_temp < 30)),\n",
    "    (\"Hot (>= 30°C)\", obs_temp >= 30)\n",
    "]\n",
    "\n",
    "print(\"\\nTemperature Range Performance:\")\n",
    "for range_name, mask in temp_ranges:\n",
    "    if np.sum(mask) > 0:\n",
    "        obs_range = obs_temp[mask]\n",
    "        mod_range = mod_temp[mask]\n",
    "        mae = ms.MAE(obs_range, mod_range)\n",
    "        rmse = ms.RMSE(obs_range, mod_range)\n",
    "        mbe = ms.MBE(obs_range, mod_range)\n",
    "        corr = ms.pearson_correlation(obs_range, mod_range)\n",
    "        print(f\"{range_name}: MAE={mae:.3f}, RMSE={rmse:.3f}, MBE={mbe:.3f}, Corr={corr:.3f}, Count={np.sum(mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precipitation Model-Observation Comparison\n",
    "\n",
    "Analyze precipitation forecast performance using both continuous and categorical metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract precipitation data\n",
    "obs_precip = precip_df['observed_precip'].values\n",
    "mod_precip = precip_df['modeled_precip'].values\n",
    "obs_binary = precip_df['obs_binary_precip'].values\n",
    "mod_binary = precip_df['mod_binary_precip'].values\n",
    "\n",
    "# Calculate precipitation metrics\n",
    "precip_metrics = {\n",
    "    'MAE': ms.MAE(obs_precip, mod_precip),\n",
    "    'RMSE': ms.RMSE(obs_precip, mod_precip),\n",
    "    'MBE': ms.MBE(obs_precip, mod_precip),\n",
    "    'Correlation': ms.pearson_correlation(obs_precip, mod_precip),\n",
    "    'R2': ms.R2(obs_precip, mod_precip)\n",
    "}\n",
    "\n",
    "print(\"Precipitation Model Performance Metrics (Continuous):\")\n",
    "for metric, value in precip_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Calculate contingency table metrics\n",
    "print(\"\\nBinary Precipitation Metrics:\")\n",
    "print(f\"Accuracy: {ms.accuracy(obs_binary, mod_binary):.4f}\")\n",
    "print(f\"POD (Probability of Detection): {ms.POD(obs_binary, mod_binary):.4f}\")\n",
    "print(f\"FAR (False Alarm Ratio): {ms.FAR(obs_binary, mod_binary):.4f}\")\n",
    "print(f\"CSI (Critical Success Index): {ms.CSI(obs_binary, mod_binary):.4f}\")\n",
    "print(f\"Heidke Skill Score: {ms.HSS(obs_binary, mod_binary):.4f}\")\n",
    "print(f\"Brier Score: {ms.BS(obs_binary, mod_precip/np.max(mod_precip)):.4f}\")\n",
    "\n",
    "# Calculate precipitation-specific metrics\n",
    "print(\"\\nRainfall Intensity Metrics:\")\n",
    "print(f\"Mean Observed Rainfall: {np.mean(obs_precip):.4f} mm/day\")\n",
    "print(f\"Mean Modeled Rainfall: {np.mean(mod_precip):.4f} mm/day\")\n",
    "print(f\"Bias Ratio: {np.mean(mod_precip) / np.mean(obs_precip):.4f}\")\n",
    "\n",
    "# Calculate metrics for wet days only (>0.1mm)\n",
    "wet_days = obs_precip > 0.1\n",
    "if np.sum(wet_days) > 0:\n",
    "    obs_wet = obs_precip[wet_days]\n",
    "    mod_wet = mod_precip[wet_days]\n",
    "    print(f\"Wet Day RMSE: {ms.RMSE(obs_wet, mod_wet):.4f}\")\n",
    "    print(f\"Wet Day Correlation: {ms.pearson_correlation(obs_wet, mod_wet):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind Model-Observation Comparison\n",
    "\n",
    "Analyze wind speed and direction forecast performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract wind data\n",
    "obs_wind_speed = wind_df['observed_wind_speed'].values\n",
    "mod_wind_speed = wind_df['modeled_wind_speed'].values\n",
    "obs_wind_dir = wind_df['observed_wind_dir'].values\n",
    "mod_wind_dir = wind_df['modeled_wind_dir'].values\n",
    "\n",
    "# Calculate wind speed metrics\n",
    "wind_speed_metrics = {\n",
    "    'MAE': ms.MAE(obs_wind_speed, mod_wind_speed),\n",
    "    'RMSE': ms.RMSE(obs_wind_speed, mod_wind_speed),\n",
    "    'MBE': ms.MBE(obs_wind_speed, mod_wind_speed),\n",
    "    'Correlation': ms.pearson_correlation(obs_wind_speed, mod_wind_speed),\n",
    "    'R2': ms.R2(obs_wind_speed, mod_wind_speed)\n",
    "}\n",
    "\n",
    "print(\"Wind Speed Model Performance Metrics:\")\n",
    "for metric, value in wind_speed_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Calculate wind direction metrics (circular statistics)\n",
    "print(\"\\nWind Direction Metrics:\")\n",
    "\n",
    "# Calculate mean absolute error for wind direction (considering circular nature)\n",
    "def circular_mae(obs_dir, mod_dir):\n",
    "    \"\"\"Calculate circular mean absolute error for wind direction\"\"\"\n",
    "    # Calculate the minimum angular difference\n",
    "    diff = np.abs(obs_dir - mod_dir)\n",
    "    diff = np.minimum(diff, 360 - diff)\n",
    "    return np.mean(diff)\n",
    "\n",
    "dir_mae = circular_mae(obs_wind_dir, mod_wind_dir)\n",
    "print(f\"Circular Mean Absolute Error (Direction): {dir_mae:.4f} degrees\")\n",
    "\n",
    "# Calculate correlation for wind speed\n",
    "print(f\"Wind Speed Correlation: {ms.pearson_correlation(obs_wind_speed, mod_wind_speed):.4f}\")\n",
    "\n",
    "# Calculate metrics for different wind speed ranges\n",
    "wind_ranges = [\n",
    "    (\"Light (0-5 m/s)\", obs_wind_speed < 5),\n",
    "    (\"Moderate (5-10 m/s)\", (obs_wind_speed >= 5) & (obs_wind_speed < 10)),\n",
    "    (\"Strong (10-15 m/s)\", (obs_wind_speed >= 10) & (obs_wind_speed < 15)),\n",
    "    (\"Very Strong (>= 15 m/s)\", obs_wind_speed >= 15)\n",
    "]\n",
    "\n",
    "print(\"\\nWind Speed Range Performance:\")\n",
    "for range_name, mask in wind_ranges:\n",
    "    if np.sum(mask) > 0:\n",
    "        obs_range = obs_wind_speed[mask]\n",
    "        mod_range = mod_wind_speed[mask]\n",
    "        mae = ms.MAE(obs_range, mod_range)\n",
    "        rmse = ms.RMSE(obs_range, mod_range)\n",
    "        mbe = ms.MBE(obs_range, mod_range)\n",
    "        corr = ms.pearson_correlation(obs_range, mod_range)\n",
    "        print(f\"{range_name}: MAE={mae:.3f}, RMSE={rmse:.3f}, MBE={mbe:.3f}, Corr={corr:.3f}, Count={np.sum(mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Model Evaluation Dashboard\n",
    "\n",
    "Create a comprehensive dashboard comparing all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dashboard\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "# Temperature scatter plot\n",
    "axes[0, 0].scatter(obs_temp[:1000], mod_temp[:1000], alpha=0.5, s=10)\n",
    "axes[0, 0].plot([obs_temp.min(), obs_temp.max()], [obs_temp.min(), obs_temp.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Observed Temperature (°C)')\n",
    "axes[0, 0].set_ylabel('Modeled Temperature (°C)')\n",
    "axes[0, 0].set_title(f'Temperature Scatter (R² = {ms.R2(obs_temp, mod_temp):.3f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precipitation scatter plot\n",
    "axes[0, 1].scatter(obs_precip[:1000], mod_precip[:1000], alpha=0.5, s=10)\n",
    "axes[0, 1].plot([obs_precip.min(), obs_precip.max()], [obs_precip.min(), obs_precip.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Observed Precipitation (mm)')\n",
    "axes[0, 1].set_ylabel('Modeled Precipitation (mm)')\n",
    "axes[0, 1].set_title(f'Precipitation Scatter (R² = {ms.R2(obs_precip, mod_precip):.3f})')\n",
    "axes[0, 1].set_xlim(0, np.percentile(obs_precip, 95))\n",
    "axes[0, 1].set_ylim(0, np.percentile(mod_precip, 95))\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Wind speed scatter plot\n",
    "axes[0, 2].scatter(obs_wind_speed[:1000], mod_wind_speed[:1000], alpha=0.5, s=10)\n",
    "axes[0, 2].plot([obs_wind_speed.min(), obs_wind_speed.max()], [obs_wind_speed.min(), obs_wind_speed.max()], 'r--', lw=2)\n",
    "axes[0, 2].set_xlabel('Observed Wind Speed (m/s)')\n",
    "axes[0, 2].set_ylabel('Modeled Wind Speed (m/s)')\n",
    "axes[0, 2].set_title(f'Wind Speed Scatter (R² = {ms.R2(obs_wind_speed, mod_wind_speed):.3f})')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Temperature time series for first station\n",
    "first_temp_station = temp_df[temp_df['station_id'] == temp_df['station_id'].iloc[0]]\n",
    "axes[1, 0].plot(first_temp_station['date'][:365], first_temp_station['observed_temp'][:365], label='Observed', alpha=0.7)\n",
    "axes[1, 0].plot(first_temp_station['date'][:365], first_temp_station['modeled_temp'][:365], label='Modeled', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Temperature (°C)')\n",
    "axes[1, 0].set_title('Temperature Time Series Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Precipitation time series for first station\n",
    "first_precip_station = precip_df[precip_df['station_id'] == precip_df['station_id'].iloc[0]]\n",
    "axes[1, 1].plot(first_precip_station['date'][:365], first_precip_station['observed_precip'][:365], label='Observed', alpha=0.7)\n",
    "axes[1, 1].plot(first_precip_station['date'][:365], first_precip_station['modeled_precip'][:365], label='Modeled', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Precipitation (mm)')\n",
    "axes[1, 1].set_title('Precipitation Time Series Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Wind speed time series for first station\n",
    "first_wind_station = wind_df[wind_df['station_id'] == wind_df['station_id'].iloc[0]]\n",
    "axes[1, 2].plot(first_wind_station['date'][:168], first_wind_station['observed_wind_speed'][:168], label='Observed', alpha=0.7)  # 1 week of hourly data\n",
    "axes[1, 2].plot(first_wind_station['date'][:168], first_wind_station['modeled_wind_speed'][:168], label='Modeled', alpha=0.7)\n",
    "axes[1, 2].set_xlabel('Date')\n",
    "axes[1, 2].set_ylabel('Wind Speed (m/s)')\n",
    "axes[1, 2].set_title('Wind Speed Time Series Comparison (1 Week)')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Temperature error histogram\n",
    "temp_errors = mod_temp - obs_temp\n",
    "axes[2, 0].hist(temp_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2, 0].set_xlabel('Temperature Error (°C)')\n",
    "axes[2, 0].set_ylabel('Frequency')\n",
    "axes[2, 0].set_title(f'Temperature Error Distribution (Mean: {np.mean(temp_errors):.3f})')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precipitation error histogram\n",
    "precip_errors = mod_precip - obs_precip\n",
    "axes[2, 1].hist(precip_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2, 1].set_xlabel('Precipitation Error (mm)')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "axes[2, 1].set_title(f'Precipitation Error Distribution (Mean: {np.mean(precip_errors):.3f})')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Wind speed error histogram\n",
    "wind_errors = mod_wind_speed - obs_wind_speed\n",
    "axes[2, 2].hist(wind_errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2, 2].set_xlabel('Wind Speed Error (m/s)')\n",
    "axes[2, 2].set_ylabel('Frequency')\n",
    "axes[2, 2].set_title(f'Wind Speed Error Distribution (Mean: {np.mean(wind_errors):.3f})')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance by Season\n",
    "\n",
    "Analyze model performance across different seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add seasonal information to temperature data\n",
    "temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
    "temp_df['month'] = temp_df['date'].dt.month\n",
    "temp_df['season'] = temp_df['month'].map({12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "                                         3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "                                         6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "                                         9: 'Fall', 10: 'Fall', 11: 'Fall'})\n",
    "\n",
    "# Calculate metrics by season\n",
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "seasonal_metrics = []\n",
    "\n",
    "for season in seasons:\n",
    "    season_data = temp_df[temp_df['season'] == season]\n",
    "    obs_season = season_data['observed_temp'].values\n",
    "    mod_season = season_data['modeled_temp'].values\n",
    "\n",
    "    if len(obs_season) > 0:\n",
    "        seasonal_metrics.append({\n",
    "            'season': season,\n",
    "            'MAE': ms.MAE(obs_season, mod_season),\n",
    "            'RMSE': ms.RMSE(obs_season, mod_season),\n",
    "            'MBE': ms.MBE(obs_season, mod_season),\n",
    "            'Correlation': ms.pearson_correlation(obs_season, mod_season),\n",
    "            'R2': ms.R2(obs_season, mod_season),\n",
    "            'Count': len(obs_season)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "seasonal_df = pd.DataFrame(seasonal_metrics)\n",
    "print(\"Seasonal Performance Metrics (Temperature):\")\n",
    "print(seasonal_df)\n",
    "\n",
    "# Visualize seasonal performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].bar(seasonal_df['season'], seasonal_df['RMSE'])\n",
    "axes[0, 0].set_xlabel('Season')\n",
    "axes[0, 0].set_ylabel('RMSE')\n",
    "axes[0, 0].set_title('RMSE by Season')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(seasonal_df['season'], seasonal_df['MAE'])\n",
    "axes[0, 1].set_xlabel('Season')\n",
    "axes[0, 1].set_ylabel('MAE')\n",
    "axes[0, 1].set_title('MAE by Season')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].bar(seasonal_df['season'], seasonal_df['Correlation'])\n",
    "axes[1, 0].set_xlabel('Season')\n",
    "axes[1, 0].set_ylabel('Correlation')\n",
    "axes[1, 0].set_title('Correlation by Season')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(seasonal_df['season'], seasonal_df['MBE'])\n",
    "axes[1, 1].set_xlabel('Season')\n",
    "axes[1, 1].set_ylabel('MBE')\n",
    "axes[1, 1].set_title('MBE by Season')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated comprehensive model-observation comparison workflows:\n",
    "\n",
    "1. **Temperature Analysis**: Continuous metrics, performance by temperature range\n",
    "2. **Precipitation Analysis**: Both continuous and categorical metrics, binary event detection\n",
    "3. **Wind Analysis**: Speed and direction metrics, performance by wind speed range\n",
    "4. **Comprehensive Dashboard**: Multi-variable visualization and comparison\n",
    "5. **Seasonal Analysis**: Performance variation across seasons\n",
    "\n",
    "These workflows provide a complete framework for evaluating atmospheric model performance across multiple variables and conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}