{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Analysis and Downscaling with Monet Stats\n",
    "\n",
    "This notebook demonstrates spatial verification metrics and downscaling workflows using Monet Stats. We'll work with gridded spatial data and explore various spatial verification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import monet_stats as ms\n",
    "\n",
    "# For xarray support\n",
    "import xarray as xr\n",
    "\n",
    "# For spatial analysis\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spatial Example Dataset\n",
    "\n",
    "We'll use the synthetic spatial temperature dataset for this analysis."
   ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spatial datasets\n",
    "obs_da = xr.open_dataset('../data/spatial_obs.nc')['observed_temp']\n",
    "mod_da = xr.open_dataset('../data/spatial_mod.nc')['modeled_temp']\n",
    "\n",
    "print(\"Observed data shape:\", obs_da.shape)\n",
    "print(\"Modeled data shape:\", mod_da.shape)\n",
    "print(\"\\nObserved data info:\")\n",
    "print(obs_da)\n",
    "print(\"\\nModeled data info:\")\n",
    "print(mod_da)\n",
    "\n",
    "# Get coordinates\n",
    "lats = obs_da.lat.values\n",
    "lons = obs_da.lon.values\n",
    "times = obs_da.time.values\n",
    "\n",
    "print(f\"\\nSpatial grid: {len(lats)} x {len(lons)} ({lats[0]:.2f} to {lats[-1]:.2f} N, {lons[0]:.2f} to {lons[-1]:.2f} E)\")\n",
    "print(f\"Time range: {pd.to_datetime(times[0])} to {pd.to_datetime(times[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Spatial Statistics\n",
    "\n",
    "Calculate basic statistics for the spatial fields."
   ]
  },
 {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic statistics for first time step\n",
    "obs_first = obs_da[0, :, :].values # First time step\n",
    "mod_first = mod_da[0, :, :].values  # First time step\n",
    "\n",
    "# Calculate basic metrics\n",
    "print(\"Basic Statistics for First Time Step:\")\n",
    "print(f\"Observed - Mean: {np.nanmean(obs_first):.3f}, Std: {np.nanstd(obs_first):.3f}\")\n",
    "print(f\"Modeled - Mean: {np.nanmean(mod_first):.3f}, Std: {np.nanstd(mod_first):.3f}\")\n",
    "print(f\"Difference - Mean: {np.nanmean(mod_first - obs_first):.3f}, Std: {np.nanstd(mod_first - obs_first):.3f}\")\n",
    "\n",
    "# Calculate spatial correlation\n",
    "obs_flat = obs_first.flatten()\n",
    "mod_flat = mod_first.flatten()\n",
    "\n",
    "# Remove NaN values\n",
    "valid_mask = ~(np.isnan(obs_flat) | np.isnan(mod_flat))\n",
    "obs_valid = obs_flat[valid_mask]\n",
    "mod_valid = mod_flat[valid_mask]\n",
    "\n",
    "spatial_corr = ms.pearson_correlation(obs_valid, mod_valid)\n",
    "print(f\"\\nSpatial Correlation: {spatial_corr:.4f}\")\n",
    "\n",
    "# Calculate basic error metrics\n",
    "print(f\"\\nBasic Error Metrics:\")\n",
    "print(f\"MAE: {ms.MAE(obs_valid, mod_valid):.4f}\")\n",
    "print(f\"RMSE: {ms.RMSE(obs_valid, mod_valid):.4f}\")\n",
    "print(f\"MBE: {ms.MBE(obs_valid, mod_valid):.4f}\")\n",
    "print(f\"RÂ²: {ms.R2(obs_valid, mod_valid):.4f}\")"
   ]
 },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Visualization\n",
    "\n",
    "Create maps to visualize the spatial patterns."
   ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial maps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Observed field\n",
    "im1 = axes[0, 0].contourf(lons, lats, obs_first, levels=20, cmap='RdYlBu_r', transform=ccrs.PlateCarree())\n",
    "axes[0, 0].set_title('Observed Temperature')\n",
    "axes[0, 0].add_feature(cfeature.COASTLINE)\n",
    "axes[0, 0].add_feature(cfeature.BORDERS)\n",
    "plt.colorbar(im1, ax=axes[0, 0], shrink=0.8)\n",
    "\n",
    "# Modeled field\n",
    "im2 = axes[0, 1].contourf(lons, lats, mod_first, levels=20, cmap='RdYlBu_r', transform=ccrs.PlateCarree())\n",
    "axes[0, 1].set_title('Modeled Temperature')\n",
    "axes[0, 1].add_feature(cfeature.COASTLINE)\n",
    "axes[0, 1].add_feature(cfeature.BORDERS)\n",
    "plt.colorbar(im2, ax=axes[0, 1], shrink=0.8)\n",
    "\n",
    "# Difference field\n",
    "diff_field = mod_first - obs_first\n",
    "im3 = axes[1, 0].contourf(lons, lats, diff_field, levels=20, cmap='RdBu_r', transform=ccrs.PlateCarree())\n",
    "axes[1, 0].set_title('Model - Observed Difference')\n",
    "axes[1, 0].add_feature(cfeature.COASTLINE)\n",
    "axes[1, 0].add_feature(cfeature.BORDERS)\n",
    "plt.colorbar(im3, ax=axes[1, 0], shrink=0.8)\n",
    "\n",
    "# Spatial correlation by latitude band\n",
    "lat_bands = []\n",
    "lat_corr = []\n",
    "for i in range(0, len(lats), 5):  # Every 5 latitude points\n",
    "    if i + 5 < len(lats):\n",
    "        lat_idx = slice(i, i+5)\n",
    "        obs_band = obs_first[lat_idx, :].flatten()\n",
    "        mod_band = mod_first[lat_idx, :].flatten()\n",
    "        \n",
    "        # Remove NaN values\n",
    "        valid_mask = ~(np.isnan(obs_band) | np.isnan(mod_band))\n",
    "        if np.sum(valid_mask) > 0:\n",
    "            obs_valid = obs_band[valid_mask]\n",
    "            mod_valid = mod_band[valid_mask]\n",
    "            \n",
    "            if len(obs_valid) > 0:\n",
    "                band_corr = ms.pearson_correlation(obs_valid, mod_valid)\n",
    "                lat_bands.append(np.mean(lats[lat_idx]))\n",
    "                lat_corr.append(band_corr)\n",
    "\n",
    "axes[1, 1].plot(lat_bands, lat_corr, 'o-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Latitude')\n",
    "axes[1, 1].set_ylabel('Correlation')\n",
    "axes[1, 1].set_title('Spatial Correlation by Latitude Band')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Verification Metrics\n",
    "\n",
    "Calculate advanced spatial verification metrics using Monet Stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spatial verification metrics for first time step\n",
    "print(\"Spatial Verification Metrics for First Time Step:\")\n",
    "\n",
    "# Calculate neighborhood-based metrics\n",
    "def calculate_spatial_metrics(obs_field, mod_field):\n",
    "    \"\"\"Calculate various spatial metrics for a 2D field\"\"\"\n",
    "    \n",
    "    # Flatten fields\n",
    "    obs_flat = obs_field.flatten()\n",
    "    mod_flat = mod_field.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~(np.isnan(obs_flat) | np.isnan(mod_flat))\n",
    "    obs_valid = obs_flat[valid_mask]\n",
    "    mod_valid = mod_flat[valid_mask]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'MAE': ms.MAE(obs_valid, mod_valid),\n",
    "        'RMSE': ms.RMSE(obs_valid, mod_valid),\n",
    "        'MBE': ms.MBE(obs_valid, mod_valid),\n",
    "        'Correlation': ms.pearson_correlation(obs_valid, mod_valid),\n",
    "        'R2': ms.R2(obs_valid, mod_valid),\n",
    "        'NSE': ms.NSE(obs_valid, mod_valid)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for first time step\n",
    "first_step_metrics = calculate_spatial_metrics(obs_first, mod_first)\n",
    "for metric, value in first_step_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Calculate metrics for multiple time steps\n",
    "time_metrics = []\n",
    "n_time_steps = min(10, len(obs_da.time))  # Use first 10 time steps\n",
    "\n",
    "for t in range(n_time_steps):\n",
    "    obs_t = obs_da[t, :, :].values\n",
    "    mod_t = mod_da[t, :, :].values\n",
    "    \n",
    "    t_metrics = calculate_spatial_metrics(obs_t, mod_t)\n",
    "    t_metrics['time_step'] = t\n",
    "    time_metrics.append(t_metrics)\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "time_df = pd.DataFrame(time_metrics)\n",
    "print(f\"\\nMetrics for First {n_time_steps} Time Steps:\")\n",
    "print(time_df[['time_step', 'RMSE', 'Correlation', 'MBE']].round(4))"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscaling Example\n",
    "\n",
    "Demonstrate a simple downscaling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple downscaling example\n",
    "def simple_spatial_downscaling(coarse_obs, coarse_mod, target_resolution):\n",
    "    \"\"\"Simple spatial downscaling using interpolation\"\"\"\n",
    "    from scipy.ndimage import zoom\n",
    "    \n",
    "    # Calculate zoom factors\n",
    "    zoom_factor = target_resolution / coarse_obs.shape[0]\n",
    "    \n",
    "    # Interpolate both fields\n",
    "    fine_obs = zoom(coarse_obs, zoom_factor, order=1)  # Linear interpolation\n",
    "    fine_mod = zoom(coarse_mod, zoom_factor, order=1)\n",
    "    \n",
    "    return fine_obs, fine_mod\n",
    "\n",
    "# Create a coarse version of the first time step\n",
    "obs_coarse = obs_first[::5, ::5]  # Take every 5th point\n",
    "mod_coarse = mod_first[::5, ::5]\n",
    "\n",
    "print(f\"Coarse resolution: {obs_coarse.shape}\")\n",
    "print(f\"Original resolution: {obs_first.shape}\")\n",
    "\n",
    "# Downscale back to original resolution\n",
    "obs_downscaled, mod_downscaled = simple_spatial_downscaling(obs_coarse, mod_coarse, obs_first.shape[0])\n",
    "\n",
    "print(f\"Downscaled resolution: {obs_downscaled.shape}\")\n",
    "\n",
    "# Calculate metrics for downscaled fields\n",
    "obs_down_flat = obs_downscaled.flatten()\n",
    "mod_down_flat = mod_downscaled.flatten()\n",
    "obs_orig_flat = obs_first.flatten()\n",
    "mod_orig_flat = mod_first.flatten()\n",
    "\n",
    "# Remove NaN values\n",
    "valid_mask = ~(np.isnan(obs_orig_flat) | np.isnan(mod_orig_flat))\n",
    "obs_orig_valid = obs_orig_flat[valid_mask]\n",
    "mod_orig_valid = mod_orig_flat[valid_mask]\n",
    "obs_down_valid = obs_down_flat[valid_mask]\n",
    "mod_down_valid = mod_down_flat[valid_mask]\n",
    "\n",
    "print(f\"\\nDownscaling Performance:\")\n",
    "print(f\"Original RMSE: {ms.RMSE(obs_orig_valid, mod_orig_valid):.4f}\")\n",
    "print(f\"Downscaled RMSE: {ms.RMSE(obs_down_valid, mod_down_valid):.4f}\")\n",
    "print(f\"Downscaled Bias Improvement: {ms.MBE(obs_down_valid, obs_orig_valid):.4f}\")\n",
    "\n",
    "# Visualize downscaling results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original observed\n",
    "im1 = axes[0, 0].imshow(obs_first, cmap='RdYlBu_r', origin='lower')\n",
    "axes[0, 0].set_title('Original Observed')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Coarse observed\n",
    "im2 = axes[0, 1].imshow(obs_coarse, cmap='RdYlBu_r', origin='lower')\n",
    "axes[0, 1].set_title('Coarse Observed')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Downscaled observed\n",
    "im3 = axes[0, 2].imshow(obs_downscaled, cmap='RdYlBu_r', origin='lower')\n",
    "axes[0, 2].set_title('Downscaled Observed')\n",
    "plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    # Original modeled\n",
    "im4 = axes[1, 0].imshow(mod_first, cmap='RdYlBu_r', origin='lower')\n",
    "axes[1, 0].set_title('Original Modeled')\n",
    "plt.colorbar(im4, ax=axes[1, 0])\n",
    "\n",
    # Coarse modeled\n",
    "im5 = axes[1, 1].imshow(mod_coarse, cmap='RdYlBu_r', origin='lower')\n",
    "axes[1, 1].set_title('Coarse Modeled')\n",
    "plt.colorbar(im5, ax=axes[1, 1])\n",
    "\n",
    # Downscaled modeled\n",
    "im6 = axes[1, 2].imshow(mod_downscaled, cmap='RdYlBu_r', origin='lower')\n",
    "axes[1, 2].set_title('Downscaled Modeled')\n",
    "plt.colorbar(im6, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Skill Scores\n",
    "\n",
    "Calculate spatial skill scores and evaluate spatial patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate spatial skill scores\n",
    "def calculate_spatial_skill(obs_field, mod_field, obs_clim=None):\n",
    "    \"\"Calculate spatial skill scores\"\"\"\n",
    "    \n",
    "    # Flatten fields\n",
    "    obs_flat = obs_field.flatten()\n",
    "    mod_flat = mod_field.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~(np.isnan(obs_flat) | np.isnan(mod_flat))\n",
    "    obs_valid = obs_flat[valid_mask]\n",
    "    mod_valid = mod_flat[valid_mask]\n",
    "    \n",
    "    # Calculate reference (climatology) if not provided\n",
    "    if obs_clim is None:\n",
    "        obs_clim = np.mean(obs_valid)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse_model = ms.RMSE(obs_valid, mod_valid)\n",
    "    rmse_clim = ms.RMSE(obs_valid, np.full_like(obs_valid, obs_clim))\n",
    "    \n",
    "    # Skill score\n",
    "    ss_rmse = ms.SS(mse_model=rmse_model**2, mse_reference=rmse_clim**2)\n",
    "    \n",
    "    # Nash-Sutcliffe Efficiency\n",
    "    nse = ms.NSE(obs_valid, mod_valid)\n",
    "    \n",
    "    return {\n",
    "        'RMSE_Skill_Score': ss_rmse,\n",
    "        'NSE': nse,\n",
    "        'Spatial_Correlation': ms.pearson_correlation(obs_valid, mod_valid),\n",
    "        'RMSE': rmse_model\n",
    "    }\n",
    "\n",
    "# Calculate skill scores for first time step\n",
    "skill_scores = calculate_spatial_skill(obs_first, mod_first)\n",
    "print(\"Spatial Skill Scores for First Time Step:\")\n",
    "for metric, value in skill_scores.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Calculate skill scores for all time steps\n",
    "all_skill_scores = []\n",
    "for t in range(len(obs_da.time)):\n",
    "    obs_t = obs_da[t, :, :].values\n",
    "    mod_t = mod_da[t, :, :].values\n",
    "    \n",
    "    t_skill = calculate_spatial_skill(obs_t, mod_t)\n",
    "    t_skill['time_step'] = t\n",
    "    t_skill['date'] = pd.to_datetime(obs_da.time[t].values)\n",
    "    all_skill_scores.append(t_skill)\n",
    "\n",
    "# Convert to DataFrame\n",
    "skill_df = pd.DataFrame(all_skill_scores)\n",
    "\n",
    "# Plot time series of skill scores\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(skill_df['date'], skill_df['RMSE_Skill_Score'], linewidth=1)\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('RMSE Skill Score')\n",
    "axes[0, 0].set_title('RMSE Skill Score Time Series')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[0, 1].plot(skill_df['date'], skill_df['NSE'], linewidth=1)\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('NSE')\n",
    "axes[0, 1].set_title('NSE Time Series')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1, 0].plot(skill_df['date'], skill_df['Spatial_Correlation'], linewidth=1)\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Spatial Correlation')\n",
    "axes[1, 0].set_title('Spatial Correlation Time Series')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1, 1].plot(skill_df['date'], skill_df['RMSE'], linewidth=1)\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('RMSE')\n",
    "axes[1, 1].set_title('RMSE Time Series')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional Analysis\n",
    "\n",
    "Perform regional analysis on different parts of the domain."
   ]
 },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions for analysis\n",
    "def define_regions(lats, lons):\n",
    "    \"\"Define regions based on lat/lon boundaries\"\"\"\n",
    "    regions = {\n",
    "        'North': {'lat': (lats.mean(), lats.max()), 'lon': (lons.min(), lons.max())},\n",
    "        'South': {'lat': (lats.min(), lats.mean()), 'lon': (lons.min(), lons.max())},\n",
    "        'East': {'lat': (lats.min(), lats.max()), 'lon': (lons.mean(), lons.max())},\n",
    "        'West': {'lat': (lats.min(), lats.max()), 'lon': (lons.min(), lons.mean())}\n",
    "    }\n",
    "    return regions\n",
    "\n",
    "regions = define_regions(lats, lons)\n",
    "\n",
    "# Calculate metrics for each region\n",
    "region_metrics = []\n",
    "for region_name, bounds in regions.items():\n",
    "    # Find indices for the region\n",
    "    lat_min, lat_max = bounds['lat']\n",
    "    lon_min, lon_max = bounds['lon']\n",
    "    \n",
    "    lat_idx = np.where((lats >= lat_min) & (lats <= lat_max))[0]\n",
    "    lon_idx = np.where((lons >= lon_min) & (lons <= lon_max))[0]\n",
    "    \n",
    "    # Extract regional data for first time step\n",
    "    obs_reg = obs_da[0, lat_idx[0]:lat_idx[-1]+1, lon_idx[0]:lon_idx[-1]+1].values\n",
    "    mod_reg = mod_da[0, lat_idx[0]:lat_idx[-1]+1, lon_idx[0]:lon_idx[-1]+1].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    obs_flat = obs_reg.flatten()\n",
    "    mod_flat = mod_reg.flatten()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_mask = ~(np.isnan(obs_flat) | np.isnan(mod_flat))\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        obs_valid = obs_flat[valid_mask]\n",
    "        mod_valid = mod_flat[valid_mask]\n",
    "        \n",
    "        region_metrics.append({\n",
    "            'region': region_name,\n",
    "            'MAE': ms.MAE(obs_valid, mod_valid),\n",
    "            'RMSE': ms.RMSE(obs_valid, mod_valid),\n",
    "            'MBE': ms.MBE(obs_valid, mod_valid),\n",
    "            'Correlation': ms.pearson_correlation(obs_valid, mod_valid),\n",
    "            'R2': ms.R2(obs_valid, mod_valid),\n",
    "            'Count': np.sum(valid_mask)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "region_df = pd.DataFrame(region_metrics)\n",
    "print(\"Regional Performance Metrics:\")\n",
    "print(region_df)\n",
    "\n",
    "# Visualize regional performance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot regional maps\n",
    "im1 = axes[0, 0].imshow(obs_first, cmap='RdYlBu_r', origin='lower')\n",
    "axes[0, 0].set_title('Full Domain - Observed')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "im2 = axes[0, 1].imshow(mod_first, cmap='RdYlBu_r', origin='lower')\n",
    "axes[0, 1].set_title('Full Domain - Modeled')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "im3 = axes[0, 2].imshow(mod_first - obs_first, cmap='RdBu_r', origin='lower')\n",
    "axes[0, 2].set_title('Full Domain - Difference')\n",
    "plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    "# Add regional boundaries\n",
    "for i, (region_name, bounds) in enumerate(regions.items()):\n",
    "    lat_min, lat_max = bounds['lat']\n",
    "    lon_min, lon_max = bounds['lon']\n",
    "    \n",
    "    # Find pixel coordinates\n",
    "    lat_min_idx = np.argmin(np.abs(lats - lat_min))\n",
    "    lat_max_idx = np.argmin(np.abs(lats - lat_max))\n",
    "    lon_min_idx = np.argmin(np.abs(lons - lon_min))\n",
    "    lon_max_idx = np.argmin(np.abs(lons - lon_max))\n",
    "    \n",
    "    # Highlight region\n",
    "    axes[1, i%3].imshow(obs_first, cmap='RdYlBu_r', origin='lower')\n",
    "    axes[1, i%3].add_patch(plt.Rectangle((lon_min_idx-0.5, lat_min_idx-0.5), \n",
    "                                        lon_max_idx-lon_min_idx, lat_max_idx-lat_min_idx, \n",
    "                                        fill=False, edgecolor='red', linewidth=2))\n",
    "    axes[1, i%3].set_title(f'{region_name} Region')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated spatial analysis and downscaling workflows:\n",
    "\n",
    "1. **Spatial Visualization**: Mapping observed and modeled fields\n",
    "2. **Spatial Verification Metrics**: Correlation, error metrics, and skill scores\n",
    "3. **Downscaling Example**: Simple interpolation-based downscaling\n",
    "4. **Regional Analysis**: Performance evaluation by geographic regions\n",
    "5. **Time Series Analysis**: Temporal evolution of spatial metrics\n",
    "\n",
    "These techniques provide a comprehensive framework for evaluating spatial model performance in atmospheric sciences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
