{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integration Examples with Monet Stats\n",
        "\n",
        "This notebook demonstrates comprehensive integration workflows combining multiple metrics and techniques using Monet Stats. We'll explore multi-dataset integration, machine learning integration, and comprehensive dashboards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xarray as xr\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "# For xarray support\n",
        "import monet_stats as ms\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration Examples\n",
        "\n",
        "We'll explore various integration workflows combining multiple metrics and techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load multiple datasets for integration examples\n",
        "temp_df = pd.read_csv('data/temperature_obs_mod.csv')\n",
        "precip_df = pd.read_csv('data/precipitation_obs_mod.csv')\n",
        "wind_df = pd.read_csv('data/wind_obs_mod.csv')\n",
        "\n",
        "print(\"Loaded datasets:\")\n",
        "print(f\"Temperature: {temp_df.shape}\")\n",
        "print(f\"Precipitation: {precip_df.shape}\")\n",
        "print(f\"Wind: {wind_df.shape}\")\n",
        "\n",
        "# Show first few rows of each dataset\n",
        "print(\"\\nTemperature dataset sample:\")\n",
        "print(temp_df.head(3))\n",
        "print(\"\\nPrecipitation dataset sample:\")\n",
        "print(precip_df.head(3))\n",
        "print(\"\\nWind dataset sample:\")\n",
        "print(wind_df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-Dataset Integration\n",
        "\n",
        "Combine metrics from multiple datasets to create comprehensive evaluation frameworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for each dataset\n",
        "def calculate_comprehensive_metrics(obs, mod, var_name):\n",
        "    \"\"\"Calculate comprehensive metrics for a variable\"\"\"\n",
        "    metrics = {\n",
        "        'Variable': var_name,\n",
        "        'MAE': ms.MAE(obs, mod),\n",
        "        'RMSE': ms.RMSE(obs, mod),\n",
        "        'Correlation': ms.pearsonr(obs, mod),\n",
        "        'NSE': ms.NSE(obs, mod),\n",
        "        'MB': ms.MB(obs, mod),\n",
        "        'R2': ms.R2(obs, mod),\n",
        "        'Count': len(obs)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for each variable\n",
        "temp_metrics = calculate_comprehensive_metrics(\n",
        "    temp_df['observed_temp'].values, \n",
        "    temp_df['modeled_temp'].values, \n",
        "    'Temperature'\n",
        ")\n",
        "\n",
        "precip_metrics = calculate_comprehensive_metrics(\n",
        "    precip_df['observed_precip'].values, \n",
        "    precip_df['modeled_precip'].values, \n",
        "    'Precipitation'\n",
        ")\n",
        "\n",
        "wind_metrics = calculate_comprehensive_metrics(\n",
        "    wind_df['observed_wind_speed'].values, \n",
        "    wind_df['modeled_wind_speed'].values, \n",
        "    'Wind Speed'\n",
        ")\n",
        "\n",
        "# Combine metrics into a DataFrame\n",
        "all_metrics_df = pd.DataFrame([temp_metrics, precip_metrics, wind_metrics])\n",
        "print(\"Comprehensive Metrics Across Variables:\")\n",
        "print(all_metrics_df.round(4))\n",
        "\n",
        "# Visualize multi-dataset metrics\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# MAE comparison\n",
        "axes[0, 0].bar(all_metrics_df['Variable'], all_metrics_df['MAE'])\n",
        "axes[0, 0].set_title('Mean Absolute Error by Variable')\n",
        "axes[0, 0].set_ylabel('MAE')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# RMSE comparison\n",
        "axes[0, 1].bar(all_metrics_df['Variable'], all_metrics_df['RMSE'])\n",
        "axes[0, 1].set_title('Root Mean Square Error by Variable')\n",
        "axes[0, 1].set_ylabel('RMSE')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Correlation comparison\n",
        "axes[0, 2].bar(all_metrics_df['Variable'], all_metrics_df['Correlation'])\n",
        "axes[0, 2].set_title('Correlation by Variable')\n",
        "axes[0, 2].set_ylabel('Correlation')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# NSE comparison\n",
        "axes[1, 0].bar(all_metrics_df['Variable'], all_metrics_df['NSE'])\n",
        "axes[1, 0].set_title('Nash-Sutcliffe Efficiency by Variable')\n",
        "axes[1, 0].set_ylabel('NSE')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Bias comparison\n",
        "axes[1, 1].bar(all_metrics_df['Variable'], all_metrics_df['MB'])\n",
        "axes[1, 1].set_title('Mean Bias by Variable')\n",
        "axes[1, 1].set_ylabel('Bias')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# R2 comparison\n",
        "axes[1, 2].bar(all_metrics_df['Variable'], all_metrics_df['R2'])\n",
        "axes[1, 2].set_title('R² by Variable')\n",
        "axes[1, 2].set_ylabel('R²')\n",
        "axes[1, 2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Integration\n",
        "\n",
        "Integrate Monet Stats metrics with machine learning models for enhanced evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a synthetic dataset combining multiple variables for ML\n",
        "# We'll create features from the existing datasets\n",
        "combined_data = pd.DataFrame()\n",
        "\n",
        "# Use a subset of data for computational efficiency\n",
        "n_samples = min(len(temp_df), len(precip_df), len(wind_df))\n",
        "n_samples = min(n_samples, 10000)  # Limit for computational efficiency\n",
        "\n",
        "# Create combined features\n",
        "combined_data['temp_obs'] = temp_df['observed_temp'].values[:n_samples]\n",
        "combined_data['temp_mod'] = temp_df['modeled_temp'].values[:n_samples]\n",
        "combined_data['precip_obs'] = precip_df['observed_precip'].values[:n_samples]\n",
        "combined_data['precip_mod'] = precip_df['modeled_precip'].values[:n_samples]\n",
        "combined_data['wind_obs'] = wind_df['observed_wind_speed'].values[:n_samples]\n",
        "combined_data['wind_mod'] = wind_df['modeled_wind_speed'].values[:n_samples]\n",
        "\n",
        "# Create target variable (e.g., combined error metric)\n",
        "temp_error = np.abs(combined_data['temp_obs'] - combined_data['temp_mod'])\n",
        "precip_error = np.abs(combined_data['precip_obs'] - combined_data['precip_mod'])\n",
        "wind_error = np.abs(combined_data['wind_obs'] - combined_data['wind_mod'])\n",
        "\n",
        "# Combined error as target\n",
        "combined_data['target'] = (temp_error + precip_error + wind_error) / 3\n",
        "\n",
        "print(f\"Combined dataset shape: {combined_data.shape}\")\n",
        "print(\"Combined dataset sample:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Prepare features and target\n",
        "feature_cols = ['temp_obs', 'temp_mod', 'precip_obs', 'precip_mod', 'wind_obs', 'wind_mod']\n",
        "X = combined_data[feature_cols]\n",
        "y = combined_data['target']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate with standard ML metrics\n",
        "ml_mse = mean_squared_error(y_test, y_pred)\n",
        "ml_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Evaluate with monet_stats metrics\n",
        "ml_mae = ms.MAE(y_test.values, y_pred)\n",
        "ml_rmse = ms.RMSE(y_test.values, y_pred)\n",
        "ml_corr = ms.pearsonr(y_test.values, y_pred)\n",
        "ml_nse = ms.NSE(y_test.values, y_pred)\n",
        "\n",
        "print(f\"\\nMachine Learning Model Performance:\")\n",
        "print(f\"MSE: {ml_mse:.4f}\")\n",
        "print(f\"R²: {ml_r2:.4f}\")\n",
        "print(f\"MAE (Monet Stats): {ml_mae:.4f}\")\n",
        "print(f\"RMSE (Monet Stats): {ml_rmse:.4f}\")\n",
        "print(f\"Correlation (Monet Stats): {ml_corr:.4f}\")\n",
        "print(f\"NSE (Monet Stats): {ml_nse:.4f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(f\"\\nFeature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Plot predictions vs actual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "axes[0].scatter(y_test, y_pred, alpha=0.5)\n",
        "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Combined Error')\n",
        "axes[0].set_ylabel('Predicted Combined Error')\n",
        "axes[0].set_title(f'ML Predictions vs Actual (R² = {ml_r2:.3f})')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].bar(feature_importance['Feature'], feature_importance['Importance'])\n",
        "axes[1].set_xlabel('Feature')\n",
        "axes[1].set_ylabel('Importance')\n",
        "axes[1].set_title('Feature Importance in Combined Error Prediction')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Integration: Ensemble Model with Monet Stats\n",
        "\n",
        "Create an ensemble model that combines multiple evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an ensemble evaluation framework\n",
        "class ModelEvaluator:\n",
        "    def __init__(self):\n",
        "        self.metrics = {}\n",
        "    \n",
        "    def evaluate_model(self, obs, mod, model_name):\n",
        "        \"\"\"Evaluate a model using multiple metrics\"\"\"\n",
        "        results = {\n",
        "            'Model': model_name,\n",
        "            'MAE': ms.MAE(obs, mod),\n",
        "            'RMSE': ms.RMSE(obs, mod),\n",
        "            'Correlation': ms.pearsonr(obs, mod),\n",
        "            'NSE': ms.NSE(obs, mod),\n",
        "            'MB': ms.MB(obs, mod),\n",
        "            'R2': ms.R2(obs, mod),\n",
        "            'IOA': ms.IOA(obs, mod),\n",
        "            'WDIOA': ms.WDIOA(obs, mod),\n",
        "            'KGE': ms.KGE(obs, mod)\n",
        "        }\n",
        "        return results\n",
        "    \n",
        "    def compare_models(self, obs, model_predictions):\n",
        "        \"\"\"Compare multiple models\"\"\"\n",
        "        results = []\n",
        "        for model_name, predictions in model_predictions.items():\n",
        "            result = self.evaluate_model(obs, predictions, model_name)\n",
        "            results.append(result)\n",
        "        return pd.DataFrame(results)\n",
        "    \n",
        "    def create_ensemble_prediction(self, model_predictions, weights=None):\n",
        "        \"\"\"Create ensemble prediction from multiple models\"\"\"\n",
        "        if weights is None:\n",
        "            # Equal weights\n",
        "            weights = np.ones(len(model_predictions)) / len(model_predictions)\n",
        "        \n",
        "        # Normalize weights\n",
        "        weights = np.array(weights) / np.sum(weights)\n",
        "        \n",
        "        # Create ensemble prediction\n",
        "        ensemble_pred = None\n",
        "        for i, (model_name, pred) in enumerate(model_predictions.items()):\n",
        "            if ensemble_pred is None:\n",
        "                ensemble_pred = weights[i] * pred\n",
        "            else:\n",
        "                ensemble_pred += weights[i] * pred\n",
        "        \n",
        "        return ensemble_pred\n",
        "\n",
        "# Create synthetic model predictions for demonstration\n",
        "n_samples = len(temp_df)\n",
        "observed = temp_df['observed_temp'].values\n",
        "\n",
        "# Create different model predictions with varying quality\n",
        "model_predictions = {\n",
        "    'Model_1_Basic': temp_df['modeled_temp'].values,  # Original model\n",
        "    'Model_2_Biased': temp_df['modeled_temp'].values + 2.0,  # Positive bias\n",
        "    'Model_3_Noisy': temp_df['modeled_temp'].values + np.random.normal(0, 1, n_samples),  # Added noise\n",
        "    'Model_4_Accurate': temp_df['observed_temp'].values + np.random.normal(0, 0.5, n_samples) # High accuracy\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "evaluator = ModelEvaluator()\n",
        "model_comparison = evaluator.compare_models(observed, model_predictions)\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(model_comparison.round(4))\n",
        "\n",
        "# Create ensemble prediction (using simple average)\n",
        "ensemble_pred = evaluator.create_ensemble_prediction(model_predictions)\n",
        "ensemble_result = evaluator.evaluate_model(observed, ensemble_pred, 'Ensemble')\n",
        "\n",
        "print(f\"\\nEnsemble Model Performance:\")\n",
        "ensemble_series = pd.Series(ensemble_result)\n",
        "print(ensemble_series[['Model', 'MAE', 'RMSE', 'Correlation', 'NSE', 'R2']])\n",
        "\n",
        "# Compare ensemble to individual models\n",
        "ensemble_comparison = pd.concat([model_comparison, pd.DataFrame([ensemble_result])], ignore_index=True)\n",
        "best_mae_model = ensemble_comparison.loc[ensemble_comparison['MAE'].idxmin(), 'Model']\n",
        "best_rmse_model = ensemble_comparison.loc[ensemble_comparison['RMSE'].idxmin(), 'Model']\n",
        "best_corr_model = ensemble_comparison.loc[ensemble_comparison['Correlation'].idxmax(), 'Model']\n",
        "\n",
        "print(f\"\\nBest performing models:\")\n",
        "print(f\"By MAE: {best_mae_model}\")\n",
        "print(f\"By RMSE: {best_rmse_model}\")\n",
        "print(f\"By Correlation: {best_corr_model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Dashboard Creation\n",
        "\n",
        "Create an integrated dashboard combining multiple evaluation techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive dashboard function\n",
        "def create_comprehensive_dashboard(temp_df, precip_df, wind_df):\n",
        "    \"\"\"Create a comprehensive evaluation dashboard\"\"\"\n",
        "    \n",
        "    # Calculate metrics for each variable\n",
        "    temp_obs = temp_df['observed_temp'].values\n",
        "    temp_mod = temp_df['modeled_temp'].values\n",
        "    \n",
        "    precip_obs = precip_df['observed_precip'].values\n",
        "    precip_mod = precip_df['modeled_precip'].values\n",
        "    \n",
        "    wind_obs = wind_df['observed_wind_speed'].values\n",
        "    wind_mod = wind_df['modeled_wind_speed'].values\n",
        "    \n",
        "    # Calculate comprehensive metrics\n",
        "    temp_metrics = {\n",
        "        'Variable': 'Temperature',\n",
        "        'MAE': ms.MAE(temp_obs, temp_mod),\n",
        "        'RMSE': ms.RMSE(temp_obs, temp_mod),\n",
        "        'Correlation': ms.pearsonr(temp_obs, temp_mod),\n",
        "        'NSE': ms.NSE(temp_obs, temp_mod),\n",
        "        'MB': ms.MB(temp_obs, temp_mod),\n",
        "        'R2': ms.R2(temp_obs, temp_mod)\n",
        "    }\n",
        "    \n",
        "    precip_metrics = {\n",
        "        'Variable': 'Precipitation',\n",
        "        'MAE': ms.MAE(precip_obs, precip_mod),\n",
        "        'RMSE': ms.RMSE(precip_obs, precip_mod),\n",
        "        'Correlation': ms.pearsonr(precip_obs, precip_mod),\n",
        "        'NSE': ms.NSE(precip_obs, precip_mod),\n",
        "        'MB': ms.MB(precip_obs, precip_mod),\n",
        "        'R2': ms.R2(precip_obs, precip_mod)\n",
        "    }\n",
        "    \n",
        "    wind_metrics = {\n",
        "        'Variable': 'Wind Speed',\n",
        "        'MAE': ms.MAE(wind_obs, wind_mod),\n",
        "        'RMSE': ms.RMSE(wind_obs, wind_mod),\n",
        "        'Correlation': ms.pearsonr(wind_obs, wind_mod),\n",
        "        'NSE': ms.NSE(wind_obs, wind_mod),\n",
        "        'MB': ms.MB(wind_obs, wind_mod),\n",
        "        'R2': ms.R2(wind_obs, wind_mod)\n",
        "    }\n",
        "    \n",
        "    metrics_df = pd.DataFrame([temp_metrics, precip_metrics, wind_metrics])\n",
        "    \n",
        "    # Create dashboard figure\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    \n",
        "    # Overall metrics comparison\n",
        "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # MAE comparison\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.bar(metrics_df['Variable'], metrics_df['MAE'])\n",
        "    ax1.set_title('Mean Absolute Error')\n",
        "    ax1.set_ylabel('MAE')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # RMSE comparison\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax2.bar(metrics_df['Variable'], metrics_df['RMSE'])\n",
        "    ax2.set_title('Root Mean Square Error')\n",
        "    ax2.set_ylabel('RMSE')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Correlation comparison\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    ax3.bar(metrics_df['Variable'], metrics_df['Correlation'])\n",
        "    ax3.set_title('Correlation')\n",
        "    ax3.set_ylabel('Correlation')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # NSE comparison\n",
        "    ax4 = fig.add_subplot(gs[0, 3])\n",
        "    ax4.bar(metrics_df['Variable'], metrics_df['NSE'])\n",
        "    ax4.set_title('Nash-Sutcliffe Efficiency')\n",
        "    ax4.set_ylabel('NSE')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Scatter plots for each variable\n",
        "    # Temperature\n",
        "    ax5 = fig.add_subplot(gs[1, :2])\n",
        "    ax5.scatter(temp_obs[:2000], temp_mod[:2000], alpha=0.5, s=10)\n",
        "    ax5.plot([temp_obs.min(), temp_obs.max()], [temp_obs.min(), temp_obs.max()], 'r--', lw=2)\n",
        "    ax5.set_xlabel('Observed Temperature')\n",
        "    ax5.set_ylabel('Modeled Temperature')\n",
        "    ax5.set_title(f'Temperature: R² = {ms.R2(temp_obs, temp_mod):.3f}, RMSE = {ms.RMSE(temp_obs, temp_mod):.3f}')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Precipitation\n",
        "    ax6 = fig.add_subplot(gs[1, 2:])\n",
        "    ax6.scatter(precip_obs[:2000], precip_mod[:2000], alpha=0.5, s=10)\n",
        "    ax6.plot([precip_obs.min(), precip_obs.max()], [precip_obs.min(), precip_obs.max()], 'r--', lw=2)\n",
        "    ax6.set_xlabel('Observed Precipitation')\n",
        "    ax6.set_ylabel('Modeled Precipitation')\n",
        "    ax6.set_title(f'Precipitation: R² = {ms.R2(precip_obs, precip_mod):.3f}, RMSE = {ms.RMSE(precip_obs, precip_mod):.3f}')\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Wind Speed\n",
        "    ax7 = fig.add_subplot(gs[2, 1:3])\n",
        "    ax7.scatter(wind_obs[:2000], wind_mod[:2000], alpha=0.5, s=10)\n",
        "    ax7.plot([wind_obs.min(), wind_obs.max()], [wind_obs.min(), wind_obs.max()], 'r--', lw=2)\n",
        "    ax7.set_xlabel('Observed Wind Speed')\n",
        "    ax7.set_ylabel('Modeled Wind Speed')\n",
        "    ax7.set_title(f'Wind Speed: R² = {ms.R2(wind_obs, wind_mod):.3f}, RMSE = {ms.RMSE(wind_obs, wind_mod):.3f}')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Error distributions\n",
        "    ax8 = fig.add_subplot(gs[3, 0])\n",
        "    temp_errors = temp_mod - temp_obs\n",
        "    ax8.hist(temp_errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax8.set_xlabel('Temperature Error')\n",
        "    ax8.set_ylabel('Frequency')\n",
        "    ax8.set_title('Temperature Error Distribution')\n",
        "    ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax9 = fig.add_subplot(gs[3, 1])\n",
        "    precip_errors = precip_mod - precip_obs\n",
        "    ax9.hist(precip_errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax9.set_xlabel('Precipitation Error')\n",
        "    ax9.set_ylabel('Frequency')\n",
        "    ax9.set_title('Precipitation Error Distribution')\n",
        "    ax9.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax10 = fig.add_subplot(gs[3, 2])\n",
        "    wind_errors = wind_mod - wind_obs\n",
        "    ax10.hist(wind_errors, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax10.set_xlabel('Wind Speed Error')\n",
        "    ax10.set_ylabel('Frequency')\n",
        "    ax10.set_title('Wind Speed Error Distribution')\n",
        "    ax10.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Summary statistics table\n",
        "    ax11 = fig.add_subplot(gs[3, 3])\n",
        "    ax11.axis('tight')\n",
        "    ax1.axis('off')\n",
        "    \n",
        "    # Format the table\n",
        "    cell_text = []\n",
        "    for row in range(len(metrics_df)):\n",
        "        cell_text.append([\n",
        "            f\"{metrics_df.iloc[row]['MAE']:.3f}\",\n",
        "            f\"{metrics_df.iloc[row]['RMSE']:.3f}\",\n",
        "            f\"{metrics_df.iloc[row]['Correlation']:.3f}\",\n",
        "            f\"{metrics_df.iloc[row]['NSE']:.3f}\"\n",
        "        ])\n",
        "    \n",
        "    table = ax11.table(\n",
        "        cellText=cell_text,\n",
        "        rowLabels=metrics_df['Variable'],\n",
        "        colLabels=['MAE', 'RMSE', 'Corr', 'NSE'],\n",
        "        cellLoc='center',\n",
        "        loc='center'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(9)\n",
        "    table.scale(1.2, 1.5)\n",
        "    ax11.set_title('Summary Table')\n",
        "    \n",
        "    plt.suptitle('Comprehensive Model Evaluation Dashboard', fontsize=16, y=0.98)\n",
        "    plt.show()\n",
        "    \n",
        "    return metrics_df\n",
        "\n",
        "# Create the comprehensive dashboard\n",
        "dashboard_metrics = create_comprehensive_dashboard(temp_df, precip_df, wind_df)\n",
        "print(\"Comprehensive Dashboard Metrics:\")\n",
        "print(dashboard_metrics.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration with External Tools\n",
        "\n",
        "Demonstrate integration with external tools and libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Integration with xarray for gridded data\n",
        "try:\n",
        "    # Load spatial datasets\n",
        "    obs_da = xr.open_dataset('data/spatial_obs.nc')['__xarray_dataarray_variable__']\n",
        "    mod_da = xr.open_dataset('data/spatial_mod.nc')['__xarray_dataarray_variable__']\n",
        "    \n",
        "    print(f\"Loaded spatial datasets:\")\n",
        "    print(f\"Observed: {obs_da.shape}\")\n",
        "    print(f\"Modeled: {mod_da.shape}\")\n",
        "    \n",
        "    # Calculate spatial metrics\n",
        "    # Flatten the data for metric calculation\n",
        "    obs_flat = obs_da.values.flatten()\n",
        "    mod_flat = mod_da.values.flatten()\n",
        "    \n",
        "    # Remove NaN values\n",
        "    mask = ~(np.isnan(obs_flat) | np.isnan(mod_flat))\n",
        "    obs_clean = obs_flat[mask]\n",
        "    mod_clean = mod_flat[mask]\n",
        "    \n",
        "    # Calculate metrics using monet_stats\n",
        "    xarray_metrics = {\n",
        "        'MAE': ms.MAE(obs_clean, mod_clean),\n",
        "        'RMSE': ms.RMSE(obs_clean, mod_clean),\n",
        "        'Correlation': ms.pearsonr(obs_clean, mod_clean),\n",
        "        'NSE': ms.NSE(obs_clean, mod_clean),\n",
        "        'MB': ms.MB(obs_clean, mod_clean)\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nXarray Integration Metrics:\")\n",
        "    for metric, value in xarray_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "        \n",
        "    # Visualization of spatial data\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # First time step of observed\n",
        "    im1 = axes[0].pcolormesh(obs_da.lon, obs_da.lat, obs_da.isel(time=0), shading='auto')\n",
        "    axes[0].set_title('Observed Temperature (First Time Step)')\n",
        "    axes[0].set_xlabel('Longitude')\n",
        "    axes[0].set_ylabel('Latitude')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "    \n",
        "    # First time step of modeled\n",
        "    im2 = axes[1].pcolormesh(mod_da.lon, mod_da.lat, mod_da.isel(time=0), shading='auto')\n",
        "    axes[1].set_title('Modeled Temperature (First Time Step)')\n",
        "    axes[1].set_xlabel('Longitude')\n",
        "    axes[1].set_ylabel('Latitude')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "    \n",
        "    # Difference\n",
        "    diff = mod_da.isel(time=0) - obs_da.isel(time=0)\n",
        "    im3 = axes[2].pcolormesh(mod_da.lon, mod_da.lat, diff, shading='auto', cmap='RdBu')\n",
        "    axes[2].set_title('Difference (Modeled - Observed)')\n",
        "    axes[2].set_xlabel('Longitude')\n",
        "    axes[2].set_ylabel('Latitude')\n",
        "    plt.colorbar(im3, ax=axes[2])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"Spatial datasets not found, skipping xarray integration example\")\n",
        "\n",
        "# Summary of integration capabilities\n",
        "print(f\"\\nIntegration Summary:\")\n",
        "print(\"1. Multi-dataset integration: Combined metrics across temperature, precipitation, and wind\")\n",
        "print(\"2. Machine learning integration: Used monet_stats metrics with ML models\")\n",
        "print(\"3. Ensemble modeling: Combined multiple models with different metrics\")\n",
        "print(\"4. Dashboard creation: Comprehensive visualization of model performance\")\n",
        "print(\"5. Xarray integration: Gridded data analysis with monet_stats metrics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration Examples Summary\n",
        "\n",
        "This notebook demonstrated comprehensive integration workflows with Monet Stats:\n",
        "\n",
        "1. **Multi-Dataset Integration**: Combined metrics from temperature, precipitation, and wind datasets for comprehensive evaluation.\n",
        "2. **Machine Learning Integration**: Integrated Monet Stats metrics with Random Forest models for enhanced evaluation.\n",
        "3. **Ensemble Modeling**: Created ensemble evaluation frameworks combining multiple models and metrics.\n",
        "4. **Dashboard Creation**: Built comprehensive visualization dashboards for model evaluation.\n",
        "5. **External Tool Integration**: Demonstrated integration with xarray for gridded data analysis.\n",
        "\n",
        "These integration examples show how Monet Stats can be combined with other tools and techniques to create powerful evaluation and analysis workflows for atmospheric models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}